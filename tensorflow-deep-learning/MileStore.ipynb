{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec1718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
    "# !ls pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5179eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9369cacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de15cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91951fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create function to read data \n",
    "def get_lines(filename: str):\n",
    "    \"\"\"\n",
    "    Reads filename (a text filename) and returns the lines of text from input file name\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        return file.readlines()\n",
    "        \n",
    "        \n",
    "train_lines  = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979862e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a70777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "  Takes in filename, reads its contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentence,\n",
    "  how many sentences are in the current abstract and what sentence number\n",
    "  the target line is.\n",
    "\n",
    "  Args:\n",
    "      filename: a string of the target text file to read and extract line data\n",
    "      from.\n",
    "\n",
    "  Returns:\n",
    "      A list of dictionaries each containing a line from an abstract,\n",
    "      the lines label, the lines position in the abstract and the total number\n",
    "      of lines in the abstract where the line is from. For example:\n",
    "\n",
    "      [{\"target\": 'CONCLUSION',\n",
    "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "        \"line_number\": 8,\n",
    "        \"total_lines\": 8}]\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "  \n",
    "  # Loop through each line in target file\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset abstract string\n",
    "    elif line.isspace(): # check to see if line is a new line\n",
    "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "      # Iterate through each line in abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {} # create empty dict to store data from line\n",
    "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "        line_data[\"target\"] = target_text_split[0] # get target label\n",
    "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "    \n",
    "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "      abstract_lines += line\n",
    "  \n",
    "  return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7967ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from file and preprocess it\n",
    "# %%time\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c5df41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432f4b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da19244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24152536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.total_lines.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ab327",
   "metadata": {},
   "source": [
    "# Get list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553066da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists \n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639d916",
   "metadata": {},
   "source": [
    "# Make numeric labels\n",
    "\n",
    "Create one hot and label encoded labels.\n",
    "To numerically encode label we'll use Scikit-Learn's OneHotEncoder and LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403f4c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# tranning labels\n",
    "train_labels_one_hot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4d999",
   "metadata": {},
   "source": [
    "# Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ce7b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af56d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance \n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac2a57",
   "metadata": {},
   "source": [
    "# Createing a series of model experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fd068",
   "metadata": {},
   "source": [
    "# 0. Getting a baseline\n",
    "\n",
    " - First model we'll be a TF-IDF Multinomial Naive Bayes as recommended.\n",
    " - Create a Scikit-Learn Pipeline which use:\n",
    "     + TfidfVectorizer to convert our abstract sentences to numbers using the TF-IDF algorithm.\n",
    "     + learns to classify our sentences using the MultinomialNB algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ade8d897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the traning data\n",
    "model_0.fit(X=train_sentences, \n",
    "           y=train_labels_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef89b7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseBase line model\n",
    "\n",
    "model_0.score(X=val_sentences, y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b544447",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = model_0.predict(X=val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e7b88c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_functions import calculate_results\n",
    "\n",
    "baseline_result = calculate_results(y_true=val_labels_encoded,\n",
    "                                  y_pred=baseline_preds)\n",
    "\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d75fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678b638",
   "metadata": {},
   "source": [
    "Preparing our data for deep sequence models\n",
    "\n",
    "- Create vectorization and embedding layyers.\n",
    "    - The vectorization will convert our text to numbers.\n",
    "    - The embedding layers will capture the relationships between those numbers.\n",
    "\n",
    "Since we will be turning our sentences into number, it's a good idea to figure out how many words are in each sentence.\n",
    "\n",
    "When out model goes through our sentences, it works best when they're all the same length (this is important for creating batches of the same size tensors).\n",
    "\n",
    "For example, if one sentence is eight wors long and another is 29 words long, we want to pad the eight word sentence with zores so it ends up being the same length as the 29 word sentence.\n",
    "\n",
    "Looks like the vast majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca143c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]  # list of len from train_sentence\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len # avenger len train sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344b3319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.25846e+05, 4.78220e+04, 5.37600e+03, 7.86000e+02, 1.46000e+02,\n",
       "        3.20000e+01, 1.90000e+01, 8.00000e+00, 3.00000e+00, 2.00000e+00]),\n",
       " array([  1. ,  30.5,  60. ,  89.5, 119. , 148.5, 178. , 207.5, 237. ,\n",
       "        266.5, 296. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of train sentence\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938668ff",
   "metadata": {},
   "source": [
    "Looks like the vast majority of sentences are between 0 and 50 tokens in length.\n",
    "\n",
    "We can use Numpy's percentile to find the value which covers 95% if the sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8ed724b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4554f99",
   "metadata": {},
   "source": [
    "When we create our tokenization layer, we will usethis value to turn all of our sentences into the same length. Meaning sentences with a length below 55 padded with zeros and sentences with a length above 55 get truncated (words after 55 get cut off).\n",
    "\n",
    "However, since hardly any sentences even come close to the max length, it would mean the majority of the data we pass to our model would be zeros ( sinces all sentences below the max length woud get padded with zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed5334",
   "metadata": {},
   "source": [
    "### Create text vectorizer\n",
    "\n",
    "Create a way to turn it into numbers.\n",
    "\n",
    "We will use the `TextVectorization` layer from TensorFlow.\n",
    "\n",
    "We will keep all the parameters default except for `max_tokens` (the number of unique words in our \n",
    "dataset) and `output_sequence_length` (our desired output length for each vectorized sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36a22ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in our vocabulary? (taken from 3.2 in https://arxiv.org/pdf/1710.06071.pdf)\n",
    "max_tokens = 68000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d033cd",
   "metadata": {},
   "source": [
    "And since discovered a sentence length of 55 covers 95% of the training sentences, we'll use that as our `output_sequence_length` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd9d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text vectorizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens= max_tokens, # number of words in vocabulary\n",
    "                                   output_sequence_length=55 # desired output length of vectorized sequences\n",
    "                                   ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80c6f1",
   "metadata": {},
   "source": [
    "let's adapt it to the trainning data (let it read the training data and figure out what number should represent what word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e669956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "130a3ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('thirteen physically active males completed two @-d supplemental periods , in a double-blind , randomized crossover design separated by a @-week washout period .',\n",
       " 24,\n",
       " <tf.Tensor: shape=(1, 55), dtype=int64, numpy=\n",
       " array([[5399, 3685,  327, 1037,  253,   51,  187, 3047, 1242,    5,    8,\n",
       "          236,   29,  484,  372, 2652,   22,    8,   89, 1611,  173,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "       dtype=int64)>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our text vectorizer\n",
    "import random \n",
    "target_sentence = random.choice(train_sentences)\n",
    "target_sentence, len(target_sentence.split()), text_vectorizer([target_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5843f",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------\n",
    "In our `text_vectorizer` object we can use:\n",
    "    \n",
    "    - method `get_vocabulary()` to find out a few different tidbits about our text.\n",
    "    \n",
    "    - method `get_config()` to find out the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0142a735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64841,\n",
       " ['', '[UNK]', 'the', 'and', 'of'],\n",
       " ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words in our training vocabulary? \n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "# look count of words in our training vocabulary, most common words and the least common words\n",
    "len(rct_20k_text_vocab), rct_20k_text_vocab[:5], rct_20k_text_vocab[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c1c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84772fa",
   "metadata": {},
   "source": [
    " ### Create custom text embedding \n",
    "\n",
    "Our `token_vectorization` layer maps the words in our text directly to numbers. However, this doesn't necessarily capture he relationships between those numbers.\n",
    "\n",
    "To create a richer numerical representation of our text, we can use an embedding.\n",
    "\n",
    "As our model learns (by going through many different examples of abstract sentences and their labels), it'll update its embedding to better represent the relationships between tokens in our corpus.\n",
    "\n",
    "We can create a trainable embedding layer using TensorFlow's `Embedding` layer.\n",
    "\n",
    "Once again, the main parameters we are concerned with here are the inputs and outputs of our Embedding layer.\n",
    "\n",
    "The `input_dim` parameter defines the size of our vocabulary. And the `output_dim` parameter defines the dimension of the embedding output.\n",
    "\n",
    "Once created, our embedding layer will take the integer outputs of our `text_vectorization` layer as inputs and convert them to feature vectors of size `output_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c458ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "thirteen physically active males completed two @-d supplemental periods , in a double-blind , randomized crossover design separated by a @-week washout period .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[5399 3685  327 1037  253   51  187 3047 1242    5    8  236   29  484\n",
      "   372 2652   22    8   89 1611  173    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.02602674  0.01222041  0.0164856  ... -0.02196503  0.04337389\n",
      "   -0.00927194]\n",
      "  [-0.02536449 -0.01161823 -0.01263221 ... -0.02235607  0.04038953\n",
      "    0.01290821]\n",
      "  [ 0.00937573  0.02584222 -0.03301819 ...  0.03798226 -0.0299369\n",
      "    0.03172499]\n",
      "  ...\n",
      "  [ 0.04108028  0.0236468  -0.03827355 ... -0.00218865 -0.03547467\n",
      "    0.03662336]\n",
      "  [ 0.04108028  0.0236468  -0.03827355 ... -0.00218865 -0.03547467\n",
      "    0.03662336]\n",
      "  [ 0.04108028  0.0236468  -0.03827355 ... -0.00218865 -0.03547467\n",
      "    0.03662336]]]\n",
      "\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create token embedding layer\n",
    "from tensorflow.keras import layers \n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vacabulary\n",
    "                              output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                              # Use masking to handle variable sequence lengths (save space)\n",
    "                              mask_zero=True,\n",
    "                              name=\"token_embedding\")\n",
    "\n",
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56edbb",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "## Create datasets\n",
    "\n",
    "We have gone through all the trouble of preprocessing our datasets to be used with a machine learning model, however, there are still a few steps we can use to make them work faster with our models.\n",
    "\n",
    "Namely, the `tf.data` API provides methods which enable faster data loading.\n",
    "\n",
    "The main steps we will want to use with our data is to turn it into a `PrefetchDataset` of batches.\n",
    "\n",
    "Doing so we will ensure TensorFlow loads our data onto the GPU as fast as possible, in turn leading to faster training time.\n",
    "\n",
    "To create a batched `PrefetchDataset` we can use the methods `batch()` and `prefetch()`, the parameter `tf.data.AUTOTUNE` will also allow TensorFlow to determine the optimal amount of compute to use to prepare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d01b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn out data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25feeb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76a10a",
   "metadata": {},
   "source": [
    "### Model 1: Conv1D with token embeddings\n",
    "\n",
    "We have now get a way to numerically represent our text and labels, time to build a series of deep models to try and improve upon our baseline.\n",
    "\n",
    "All of our deep models will follow a similar structure:\n",
    "    \n",
    "    `Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)` \n",
    "    \n",
    "The main component we will be changing throughout is the `Layers` component. Because any modern deep NLP model requires text to be converted into an embedding before meaningful patterns can be discovered within.\n",
    "\n",
    "The first model is a 1-dimensional Convolutional Neural Network. And following the standardmachine learning workflow of:\n",
    "    \n",
    "    - Build model\n",
    "    \n",
    "    - Train model\n",
    "    \n",
    "    - Evaluate model (make predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65ab9b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create 1D convolutional model to process sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embedding \n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile \n",
    "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e2fec",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "First deep sequence model build and ready to go.\n",
    "\n",
    "Checking out the model summary, you will notice the majority of the trainable parameters are within the embedding layer. If we were to increase the size of the embedding (by increasing the `output_dim` parameter of the `Embedding` layer), the number of trainable parameters would increase dramatically.\n",
    "\n",
    "It's time to fit our model to the training data but we are going to make a mindful change.\n",
    "\n",
    "Since our training data contains nearly 200,000 sentences, fitting a deep model may take a while even with a GPU. So to keep our experiments swift, we are going to run them on a subet of the training dataset.\n",
    "\n",
    "More specifically, we will only use the first 10% of batches (about 18,000 samples) of the  training set to train on and the first 10% of batches from the validation set to validate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64821255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 48s 84ms/step - loss: 0.9167 - accuracy: 0.6381 - val_loss: 0.6883 - val_accuracy: 0.7350\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 48s 85ms/step - loss: 0.6598 - accuracy: 0.7562 - val_loss: 0.6393 - val_accuracy: 0.7686\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 49s 87ms/step - loss: 0.6192 - accuracy: 0.7726 - val_loss: 0.5969 - val_accuracy: 0.7842\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(train_dataset,\n",
    "                             steps_per_epoch= int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                             epochs=3,\n",
    "                             validation_data=valid_dataset,\n",
    "                             validation_steps=int(0.1 * len(valid_dataset)),\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba0eab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step - loss: 0.5984 - accuracy: 0.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.598419189453125, 0.7867403626441956]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset (we only validated on 10% of batches during training)\n",
    "model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c89a2595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aade968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert predict probability to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44d7b9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.67403680656693,\n",
       " 'precision': 0.7833394981684093,\n",
       " 'recall': 0.7867403680656693,\n",
       " 'f1': 0.7843559401517408}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 result \n",
    "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c77760",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "### Model 2: Feature extraction with pretrained token embeddings\n",
    "\n",
    "Training our own embeddings took a little while to run, slowing our experiments down.\n",
    "\n",
    "Since we're moving towards replicating the model architecture in [*Neural Networks for Joint Sentence Classification\n",
    "in Medical Paper Abstracts*](https://arxiv.org/pdf/1612.05251.pdf), it mentions they used a [pretrained GloVe embedding](https://nlp.stanford.edu/projects/glove/) as a way to initialise their token embeddings.\n",
    "\n",
    "To emulate this, let's see what results we can get with the [pretrained Universal Sentence Encoder embeddings from TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4).\n",
    "\n",
    ">  **Note:** We could use GloVe embeddings as per the paper but since we're working with TensorFlow, we'll use what's available from TensorFlow Hub (GloVe embeddings aren't). We'll save [using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/) as an extension.\n",
    "\n",
    "The model structure will look like:\n",
    "\n",
    "` Inputs (string) -> Pretrained embeddings from TensorFlow Hub ( Universal Sentence Encoder) -> Layers -> Output (prediction probabilities)`\n",
    "\n",
    "You will notice the lack of tokenization layer we have used in a previous model. This is because the Universal Sentence Encoder (USE) take care of tokenization for us.\n",
    "\n",
    "This type of model is called transfer learning, or more specifically, feature extraction transfer learning. In other words taking the patterns a model has learned elsewhere and applyinh it to our own problem.\n",
    "\n",
    "![TensorFlow Hub Universal Feature Encoder feature extractor model we're building](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/09-model-tf-hub-USE-to-dense-layer.png)\n",
    "*The feature extractor model we're building using a pretrained embedding from TensorFlow Hub.*\n",
    "\n",
    "To download the pretrained USE into a layer we can use in our model, we can use the [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) class.\n",
    "\n",
    "We will keep the pretrained embeddings frozen (by setting `trainable=False`) and add a trainadble couple of layers on the top to tailor the model outputs to our own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee4c9867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained TensoFlow Hub USE\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                       trainable=False,\n",
    "                                       name=\"universal_sentence_encoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37167b0c",
   "metadata": {},
   "source": [
    "Beautiful, now our pretrained USE is downloaded and instantiated as a `hub.KerasLayer` instance, let is test it out on a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "931a6a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"we developed a `` pediatric advanced life support-reconstructed '' recertification course by deconstructing the training into six @-minute in situ simulation scenario sessions delivered over @ months .\",\n",
       " <tf.Tensor: shape=(30,), dtype=float32, numpy=\n",
       " array([-0.01821046,  0.02134912, -0.0583396 , -0.04529253, -0.07799703,\n",
       "         0.03063689, -0.02572216,  0.00589463,  0.01647951, -0.02022798,\n",
       "         0.06665286,  0.00391623,  0.02875149,  0.04661667,  0.04535261,\n",
       "        -0.00246954, -0.06207876,  0.06132438, -0.05601048, -0.06334782,\n",
       "         0.00870684,  0.05193744, -0.03125134,  0.01636303,  0.02240308,\n",
       "        -0.01020966, -0.03325118, -0.01297997, -0.06078208, -0.05840213],\n",
       "       dtype=float32)>,\n",
       " 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# Test out the embedding on a random sentence\n",
    "random_trainning_sentence = random.choice(train_sentences)\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_trainning_sentence])\n",
    "random_trainning_sentence, use_embedded_sentence[0][:30], len(use_embedded_sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c7fdc",
   "metadata": {},
   "source": [
    "Pretrained USE module from TensorFlow Hub takes care of tokenizing our text for us and outputs a 512 dimensional embedding vector.\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "** Building and fitting an NLP feature extraction model from TensorFlow Hub **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "379e0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extractor model using TF Hub Layer\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding \n",
    "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x) # create the output layer\n",
    "model_2 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47259fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,864,133\n",
      "Trainable params: 66,309\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d052e6",
   "metadata": {},
   "source": [
    "Checking the summary of our model we can see there's a large number of total parameters, however, the majority of these are non-trainable. This is because we set `training=False` when we instatiated our USE feature extractor layer.\n",
    "\n",
    "So when we train our model, only the top two output layers will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1fa7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 6s 7ms/step - loss: 0.9202 - accuracy: 0.6505 - val_loss: 0.7958 - val_accuracy: 0.6932\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 3s 6ms/step - loss: 0.7691 - accuracy: 0.7028 - val_loss: 0.7552 - val_accuracy: 0.7074\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 3s 6ms/step - loss: 0.7527 - accuracy: 0.7117 - val_loss: 0.7388 - val_accuracy: 0.7141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff569704c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit feature extractor model for 3 epochs\n",
    "model_2.fit(train_dataset, steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "           epochs=3,\n",
    "           validation_data=valid_dataset,\n",
    "           validation_steps=int(0.1 * len(valid_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913b00e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 5s 5ms/step - loss: 0.7412 - accuracy: 0.7138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7412346601486206, 0.7137892246246338]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset\n",
    "model_2.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9f53d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 71.37892228253673,\n",
       " 'precision': 0.7144465894612892,\n",
       " 'recall': 0.7137892228253674,\n",
       " 'f1': 0.7107138937566347}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with feature extraction model\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "# Convert the predictions with feature extraction model to classes\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "# Calculate results from TF Hb pretrained embeddings results on validation set\n",
    "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579013d",
   "metadata": {},
   "source": [
    "\n",
    "**Model 3: Conv1D with character embeddings**\n",
    "\n",
    "**Creating a character-level tokenizer**\n",
    "\n",
    "The Neural Networks for Joint Sentence Classification in Medical Paper Abstracts paper mentions their model uses a hybrid of token and character embeddings.\n",
    "\n",
    "We have build models with a custom token embedding and a pretrained token embedding, how about we build one using a character embedding?\n",
    "\n",
    "The difference between a character and token embedding is that the character embedding is created using sequences split into characters(hello -> [h,e,l,l,o]) where as a token embedding is created on sequences spilt into tokens.\n",
    "\n",
    "Token level embeddings split sequences into tokens (words) and embeddings each of them, character embeddings split sequences into characters and creates a feature vector for each.\n",
    "\n",
    "We can create a character-level embedding by first vectorizing our sequences (after they have been split into characters) using the `TextVectorization` class and them passing those vectorized sequences through an Embedding layer.\n",
    "\n",
    "Before we can vectorized our sequences on a character-level we will need to split them into characters. Let's write a function todo so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e4a03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32676a00",
   "metadata": {},
   "source": [
    "Our character-splitting function works. Let's create character-level datasets by splitting our sequence datasets into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "820510b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequence-level data into character-level data\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a58ae6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6430e86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of our sequences at character-level\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(char_lens, bins=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9dd1b7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find what character length covers 95% of sequence\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5cbda3",
   "metadata": {},
   "source": [
    "We will use that in `TextVectorization` layer as the `output_sequence_length` parameter.\n",
    "We will set `max_tokens` (the total number of different characters in our sequences) to 28, in other words, 26 letters of the alphabet + space + OOV (out of vocabulary or unknown) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13db6953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all keyboard characters for  char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3438f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    name=\"char_vectorizer\")\n",
    "\n",
    "# Adapt character vectorizer to training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9abda",
   "metadata": {},
   "source": [
    "adapted our `char_vectorizer` to out character-level sequences, check the result with `get_vocabulary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be6e4e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, ['', '[UNK]', 'e', 't', 'i'], ['k', 'x', 'z', 'q', 'j'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check character vocabylary characteristics\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "len(char_vocab), char_vocab[:5], char_vocab[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2488717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a t   b a s e l i n e   a d o l e s c e n t s   w e r e   p r e s c r i b e d   m e t f o r m i n   a n d   r a n d o m i s e d   t o   o n e   o f   t w o   e n e r g y   r e s t r i c t e d   d i e t s   .',\n",
       " 89,\n",
       " <tf.Tensor: shape=(1, 290), dtype=int64, numpy=\n",
       " array([[ 5,  3, 22,  5,  9,  2, 12,  4,  6,  2,  5, 10,  7, 12,  2,  9,\n",
       "         11,  2,  6,  3,  9, 20,  2,  8,  2, 14,  8,  2,  9, 11,  8,  4,\n",
       "         22,  2, 10, 15,  2,  3, 17,  7,  8, 15,  4,  6,  5,  6, 10,  8,\n",
       "          5,  6, 10,  7, 15,  4,  9,  2, 10,  3,  7,  7,  6,  2,  7, 17,\n",
       "          3, 20,  7,  2,  6,  2,  8, 18, 19,  8,  2,  9,  3,  8,  4, 11,\n",
       "          3,  2, 10, 10,  4,  2,  3,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]], dtype=int64)>,\n",
       " 290)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_train_chars = random.choice(train_chars)\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "random_train_chars, len(random_train_chars.split()), vectorized_chars, len(vectorized_chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf081a",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "You will notice sequences with a length shorter than 290 (output_seq_char_length) get padded with zeros on the end, this ensures all sequences passed to our model are the same length.\n",
    "\n",
    "Also, due to the `standardize` parameter of `TextVectorization` being `\"lower_and_strip_punctuation\"` and the `split` parameter being `\"whitespace\"` by default, sysbols (such as `@`) and spaces are removed.\n",
    "\n",
    "---------------------------------------------\n",
    "### Creating a character-level embedding\n",
    "\n",
    "We have got a way to vectorize our character-level sequences, now's time to create a character-level embedding. \n",
    "\n",
    "Just like our custom token embedding, we can do so using the `tensorflow.keras.layers.Embedding` class.\n",
    "\n",
    "Our character-level embedding layer requires an input demension and output dimension. The input dimension (input_dim) will be equal to the number of different characters in our `char_vocab`(28). And since we are following the structure of the model in Figure 1 of Neural Netwoks for Joint Sentence Classification in Medical Paper Abstracts, the output dimension of the character embedding (`output_dim`) will be 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "712a0793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 290, 25), dtype=float32, numpy=\n",
       "array([[[-0.02689884,  0.02409115,  0.03466495, ..., -0.02685288,\n",
       "         -0.03494097, -0.00963682],\n",
       "        [ 0.01821453, -0.04945204, -0.03118258, ...,  0.02520032,\n",
       "          0.04411751,  0.02734579],\n",
       "        [-0.02232257,  0.03401503,  0.02266918, ...,  0.02432853,\n",
       "         -0.00098894, -0.02169842],\n",
       "        ...,\n",
       "        [ 0.00684003,  0.00759763,  0.02577894, ...,  0.02109616,\n",
       "         -0.0069389 , -0.01705234],\n",
       "        [ 0.00684003,  0.00759763,  0.02577894, ...,  0.02109616,\n",
       "         -0.0069389 , -0.01705234],\n",
       "        [ 0.00684003,  0.00759763,  0.02577894, ...,  0.02109616,\n",
       "         -0.0069389 , -0.01705234]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
    "                             output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n",
    "                             mask_zero=False,\n",
    "                             name=\"char_embed\")\n",
    "char_embed(char_vectorizer([random_train_chars]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e7109",
   "metadata": {},
   "source": [
    "each of the characters in our sequences gets turned into a 25 dimension embedding.\n",
    "\n",
    "-----------------------------------------------\n",
    "### Build a Conv1D model to fit on character embeddings.\n",
    "\n",
    "We have got a way to turn our character-level sequences into numbers (`char_vectorizer`) as well as numerically represent them as an embedding (`char_embed`) let's test how efective they are at encoding the information in out sequences by creating a charater-level sequence model.\n",
    "\n",
    "The model will have the same structure as out custom token embedding model(`model_1`) except it will take character-level sequences as input instead of token-level sequences.\n",
    "\n",
    "    Input (character-level text) -> Tokenize -> Embedding -> Layers (Conv1D, GlobalMaxPool1D) -> Output (label probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c0f2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Conv1D on chars only\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        name=\"model_3_conv1D_char_embedding\")\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75215450",
   "metadata": {},
   "source": [
    "Create char-level batched with `PrefetchedDataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe980de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create char datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d59ba6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on chars only\n",
    "# Using 10% of batches\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                             steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                             epochs=3,\n",
    "                             validation_data=val_char_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_char_dataset)),\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb538f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step - loss: 0.8996 - accuracy: 0.6523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8996216654777527, 0.6523236036300659]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(val_char_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570cb13c",
   "metadata": {},
   "source": [
    "Looks like our character-level model is working, let's make some predictions with it and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc00a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([3, 1, 3, ..., 4, 4, 0], dtype=int64)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with character model only\n",
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "648010ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 65.23235800344234,\n",
       " 'precision': 0.6433533220472819,\n",
       " 'recall': 0.6523235800344234,\n",
       " 'f1': 0.6425671637553484}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Conv1D char only model results\n",
    "model_3_results = calculate_results(y_true=val_labels_encoded, \n",
    "                                   y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30e829",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "### Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer)\n",
    "\n",
    "Alright, now things are going to get spicy.\n",
    "\n",
    "In moving closer to build a model similar to the one in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it's time we tachled the hybrid token embedding layer they speak of.\n",
    "\n",
    "This hybrid token embedding layer is a combination of token embeddings and character embeddings. In other words, they create a stacked embedding to represent sequences before passing them to the sequence label prediction layer.\n",
    "\n",
    "So far we have build two models which have used token and character-level embeddings, however, these two models have used each of these embeddings exclusively.\n",
    "\n",
    "To start replicating (or getting close to replicating) the model in Figure 1, we have going to go through the following steps:\n",
    "\n",
    "    1. Create a token-level model(similar to model_1)\n",
    "    2. Create a character-level model\n",
    "    3. Combine (using layers.Concatenate) the outputs of 1 and 2\n",
    "    4. Build a series of output layers on top of 3 similar\n",
    "    5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07d4da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                            outputs=token_output)\n",
    "\n",
    "# 2. Setup character inputs/model\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                           outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,\n",
    "                                                                 char_model.output])\n",
    "\n",
    "# 4. Create output layers\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout)\n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "# 5. Construct model with char and token inputs\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                        outputs=output_layer,\n",
    "                        name=\"model_4_token_and_char_embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98d5e3",
   "metadata": {},
   "source": [
    "Get summary and plot the model to visualize.\n",
    "\n",
    "    model_4.summary()\n",
    "\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a946234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token char model\n",
    "model_4.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aee7b2",
   "metadata": {},
   "source": [
    "And again, to keep our experiments fast, we will fit our token-character-hybrid model on 10% of trainning and validate on 10% of validation batches. However, the difference with this model is that it requires two inputs, token-level sequences and character-level sequences.\n",
    "\n",
    "We can do this by create a `tf.data.Dataset` with a tuple as it's first input, for example:\n",
    "\n",
    "    - ((token_data, char_data), (label))\n",
    "    \n",
    "See in action.\n",
    "\n",
    "### Combining token and character data into a `tf.data` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbe61617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chars and tokens into a dataset\n",
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
    "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
    "\n",
    "# Prefetch and batch train data\n",
    "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "# Repeat same steps validation data\n",
    "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
    "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
    "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "059702f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>,\n",
       " <PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out training char and token embedding dataset\n",
    "train_char_token_dataset, val_char_token_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f4010",
   "metadata": {},
   "source": [
    "### Fitting a model on token and character-level sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0da0e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on tokens and chars\n",
    "model_4_history = model_4.fit(train_char_token_dataset,\n",
    "                             steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "                             validation_data=val_char_token_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_char_token_dataset)),\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c33dadd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 18s 19ms/step - loss: 0.7911 - accuracy: 0.6947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7910686135292053, 0.6946577429771423]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on the whole validation dataset\n",
    "model_4.evaluate(val_char_token_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422edf0",
   "metadata": {},
   "source": [
    "To make predictions with it, since it takes multiplie inputs, we can pass the `predict()` method a tuple of token-level sequences and character-level sequences.\n",
    "\n",
    "We can then evaluate the predictions as we have done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9d316b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 20s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 69.76698000794387,\n",
       " 'precision': 0.7090586584844231,\n",
       " 'recall': 0.6976698000794387,\n",
       " 'f1': 0.6968511077324974}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the token-character model hybrid\n",
    "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
    "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
    "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
    "                           y_pred=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49be68c",
   "metadata": {},
   "source": [
    "### Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings\n",
    "\n",
    "It seems like combining token embeddings and character embeddings gave our model a little performance boost.\n",
    "\n",
    "But there's one more piece of the puzzle we can add in.\n",
    "\n",
    "What if we engineered our own features into the model?\n",
    "\n",
    "Meaning, what if we took our own knowledge about the data and encodeed it in a numerical way to give our model more information about our samples?\n",
    "\n",
    "The precess of applying your own knowledge to build features as input to a model is called `feature engineering`.\n",
    "\n",
    "Can you think of something important about the sequences we are trying to classify? \n",
    "\n",
    "If you were to look at an abstract, would you expect the sentences to appear in order? Or does it make sense if they were to appear sequentially? For example, sequences labelled `CONCLUSIONS` at the beggining and sequences labelled `OBJECTIVE` at the end?\n",
    "\n",
    "Abstracts typically come in a sequential order, such as:\n",
    "\n",
    "    - OBJECTIVE ...\n",
    "    - METHODS ...\n",
    "    - METHODS ...\n",
    "    - METHODS ...\n",
    "    - RESULTS ...\n",
    "    - CONCLUSIONS ...\n",
    "    \n",
    "Or \n",
    "\n",
    "    - BACKGROUND ...\n",
    "    - OBJECTIVE ...\n",
    "    - METHODS ...\n",
    "    - METHODS ...\n",
    "    - RESULTS ...\n",
    "    - RESULTS ...\n",
    "    - CONCLUSIONS ...\n",
    "    - CONCLUSIONS ...\n",
    "    \n",
    "Of course, we can't engineer the sequence labels themselves into the training data (we don't have these at test time), but we can encode the order of a set of sequences in an abstract.\n",
    "\n",
    "For example,\n",
    "\n",
    "    - Sentence 1 of 10 ...\n",
    "    - Sentence 2 of 10 ...\n",
    "    - Sentence 3 of 10 ...\n",
    "    - Sentence 4 of 10 ...\n",
    "    - ...\n",
    "\n",
    "You might have noticed this when we created our `preprocess_text_with_line_numbers()` function. When we read in a text of abstracts, we counted the number of lines in an abstract as well as the number of each line itself.\n",
    "\n",
    "Doing this led to the `\"line_number\"` and `\"total_lines\"` columns of our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e2a5495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect trainning dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a7450",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "The `\"line_number\"` and `\"total_lines\"` columns are features which didn't necessarily come with the training data but can be passed to our model as a positional embedding. In other words, the positional embedding is where the sentence appears in an abstract.\n",
    "\n",
    "We can use these features because they will be available at test time.\n",
    "\n",
    "Since abstracts typically have a sequential order about them (for example, background, object, methods, results, conclusion), it makes sense to add the  line number of where a particular sentence occurs to our model. The beautiful thing is, these features will be avaiable at test time (we can just count the number of sentences in an abstract and the number of each one).\n",
    "\n",
    "Meaning, if we were to predict the labels of sequences in an abstract our model had never seen, we could count the number of lines and the track the position of each individual line and pass it to our model.\n",
    "\n",
    "### Create positional embeddings \n",
    "\n",
    "Okay, enough talk about positional embeddings, let is create them.\n",
    "\n",
    "Since our `\"line_number\"` and `\"total_line\"` columns are already numerical, we could pass them as they are to our model.\n",
    "\n",
    "But to avoid our model thinking a line with `\"line_number\"=5` is five times greater than a line with `\"line_number\"=1`, we will use one-hot-encoding to encode our `\"line_number\"` and `\"total_lines\"` features.\n",
    "\n",
    "To do this, we can use the `tf.one_hot` utility.\n",
    "\n",
    "`tf.one_hot` returns a one-hot-encoded tensor. it accepts an array (or tensor) as input and the `depth` parameter determines the dimension of the returned tensor. \n",
    "\n",
    "To figure out what we should set the `depth` parameter to, let's investigate the distribution of the `\"line_number\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "806c5670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     15000\n",
       "1     15000\n",
       "2     15000\n",
       "3     15000\n",
       "4     14992\n",
       "5     14949\n",
       "6     14758\n",
       "7     14279\n",
       "8     13346\n",
       "9     11981\n",
       "10    10041\n",
       "11     7892\n",
       "12     5853\n",
       "13     4152\n",
       "14     2835\n",
       "15     1861\n",
       "16     1188\n",
       "17      751\n",
       "18      462\n",
       "19      286\n",
       "20      162\n",
       "21      101\n",
       "22       66\n",
       "23       33\n",
       "24       22\n",
       "25       14\n",
       "26        7\n",
       "27        4\n",
       "28        3\n",
       "29        1\n",
       "30        1\n",
       "Name: line_number, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different line numbers are there\n",
    "train_df[\"line_number\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "564ab7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.line_number.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fff06",
   "metadata": {},
   "source": [
    "It looks like the majority of lines have a position of 15 or less.\n",
    "\n",
    "Knowing this, let's set the `depth` parameter of `tf.one_hot` to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4a717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38003c49",
   "metadata": {},
   "source": [
    "Setting the `depth` parameter of `tf.one_hot` to 15 means any sample with a `line_number` value of over 15 gets set to a tensor of all 0, where as any sample with a `line_number` of under 15 gets turned into a tensor of all 0 but with a 1 at the index equal to the `line_number` value\n",
    "\n",
    "\n",
    "We could create a one-hot tensor which has room for all of the potential values of `line_number` (depth=30), however, this would end up in a tensor of double the size of our current one (depth=15) where the bast majority of values are 0. Plus, only ~2.000/180.000 samples have a `line_number` value of over 15. So we would not be gaining much information about our data for doubling our feature space. This kind of problem is called the `curse of dimensionality`. However, since this we are working with deep models, it might be worth trying to throw as much information at the model as possible and seeing what happens. I will leave exploring values of the `depth` parameter as an extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72e69883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([180040, 15]),\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one-hot encoded \"line_numer\" feature sample\n",
    "train_line_numbers_one_hot.shape, train_line_numbers_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84304af1",
   "metadata": {},
   "source": [
    "with column `\"total_lines\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a54e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    24468\n",
       "10    23639\n",
       "12    22113\n",
       "9     19400\n",
       "13    18438\n",
       "14    14610\n",
       "8     12285\n",
       "15    10768\n",
       "7      7464\n",
       "16     7429\n",
       "17     5202\n",
       "6      3353\n",
       "18     3344\n",
       "19     2480\n",
       "20     1281\n",
       "5      1146\n",
       "21      770\n",
       "22      759\n",
       "23      264\n",
       "4       215\n",
       "24      200\n",
       "25      182\n",
       "26       81\n",
       "28       58\n",
       "3        32\n",
       "30       31\n",
       "27       28\n",
       "Name: total_lines, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"total_lines\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c9fbb190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"total_lines\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73940d",
   "metadata": {},
   "source": [
    "the distribution of our \"total_lines\" column that a value of 20 looks like it covers the majority of samples.\n",
    "\n",
    "We can confirm this with `np.percentile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c6656e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the coverage of a \"total_lines\" value of 20\n",
    "np.percentile(train_df.total_lines, 98) #  a value of 20 covers 98% of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37451ecd",
   "metadata": {},
   "source": [
    "Beautiful! Plenty of converage. Let's one-hot-encode our `total_lines` column just as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0e3f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([180040, 20])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Tensorflow to create one-hot-encoded tensors of our \"total_lines\" column\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# Check shape and samples of total lines one-hot tensor\n",
    "train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a56ed2",
   "metadata": {},
   "source": [
    "\n",
    "### Building  tribrid embedding model\n",
    "    \n",
    "Woohoo! Positional embedding tensors ready.\n",
    "    \n",
    "It's time to build the biggest model we have build yet. One which incorporates token embeddings, character embeddings and our newly crafted positional embeddings.\n",
    "\n",
    "We will be venturing into uncovered territory but there will be nothing here you haven't practiced before.\n",
    "\n",
    "More specifically we have going to go through the following steps:\n",
    "    \n",
    "    1. Create a token-level model (similar to model_1).\n",
    "    \n",
    "    2. Create a character-level model (similar to model_3 with a slight modification to reflect the paper)\n",
    "    \n",
    "    3. Create a `line_number` model (takes in one-hot-encoded `line_number` tensor and passes it through a non-linear layer)\n",
    "    \n",
    "    4. Create a `total_lines` model (takes in one-hot-encoded `total_lines` tensor and passes it through a non-linear layer)\n",
    "    \n",
    "    5. Combine (using `layers.Concatenate`) the outputs of 1 and 2 into a token-character-hybrid embedding and pass it series of output to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classiication in Medical Paper Abstract.\n",
    "    \n",
    "    6. Combine (using `layers.Concatenate`) the outputs of 3, 4 and 5 into a token-character-positional tribrid embedding.\n",
    "    \n",
    "    7. Create an output layer to accept the tribrid embedding and output predicted label probabilities.\n",
    "    \n",
    "    8. Combine the inputs of 1, 2, 3, 4 and outputs of 7 into a `tf.keras.Model`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dee30585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token inputs \n",
    "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                            outputs=token_outputs)\n",
    "\n",
    "# 2. Char inputs\n",
    "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                           outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Line numbers inputs\n",
    "line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                  outputs=x)\n",
    "\n",
    "# 4. Total lines inputs \n",
    "total_line_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
    "y = layers.Dense(32, activation=\"relu\")(total_line_inputs)\n",
    "total_line_model = tf.keras.Model(inputs=total_line_inputs,\n",
    "                                 outputs=y)\n",
    "\n",
    "# 5. Combine token and char embedding into a hybrid embedding\n",
    "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output,\n",
    "                                                                             char_model.output])\n",
    "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine positional embeddings with combined token and char embeddings into tribrid embedding \n",
    "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
    "                                                               total_line_model.output,\n",
    "                                                               z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "\n",
    "# 8. Put together model\n",
    "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                total_line_model.input,\n",
    "                                token_model.input,\n",
    "                                char_model.input],\n",
    "                        outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81f9e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_inputs (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_inputs (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVectoriza  (None, 290)         0           ['char_inputs[0][0]']            \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_inputs[0][0]']           \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 290, 25)      1750        ['char_vectorizer[3][0]']        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[3][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 64)          14848       ['char_embed[3][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid_embedding (C  (None, 192)         0           ['dense_8[0][0]',                \n",
      " oncatenate)                                                      'bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " line_number_input (InputLayer)  [(None, 15)]        0           []                               \n",
      "                                                                                                  \n",
      " total_lines_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 256)          49408       ['token_char_hybrid_embedding[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 32)           512         ['line_number_input[0][0]']      \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           672         ['total_lines_input[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " token_char_positional_embeddin  (None, 320)         0           ['dense_9[0][0]',                \n",
      " g (Concatenate)                                                  'dense_10[0][0]',               \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 5)            1605        ['token_char_positional_embedding\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,932,283\n",
      "Trainable params: 134,459\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe72212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf5cd6",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "\n",
    "Visualizing the model makes it much easier to understand.\n",
    "\n",
    "Essentially what we are doing is trying to encode as much information about our sequences as possible into various embeddings (the inputs to our model) so our model has the best chance to figure out what label belongs to a sequence (the outputs of our model).\n",
    "\n",
    "You will notice our model is looking very similar to the model shown in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. However, a few differences still remain:\n",
    "    \n",
    "    - We are using pretrained TensorFlow Hub token embeddings instead of GloVe embeddings.\n",
    "    \n",
    "    - We are using a Dense layer on top of our token-character hybrid embeddings instead of a bi-LSTM layer.\n",
    "    \n",
    "    - Section 3.1.3 of the paper mentions a label sequence optimization layer (which helps to make sure sequence labels come out in a respectable order) but it is not shown in Figure 1. To makeup for the lack of this layer in our model, we are created the positional embeddings layers.\n",
    "    \n",
    "    - Section 3.2 of the paper mentions the token and character embeddings are updated during trainning, our pretrained TensorFlow Hub embeddings remine frozen.\n",
    "    \n",
    "    - The paper uses the `SGD` optimizer, we are going to stick with `Adam`\n",
    "    \n",
    "All of the differences above are potential extensions of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3940d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001FFE2415548> True\n",
      "<keras.engine.input_layer.InputLayer object at 0x000001FFE206C548> True\n",
      "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x000001FF6D371D88> True\n",
      "<tensorflow_hub.keras_layer.KerasLayer object at 0x000001FF428A6D08> False\n",
      "<keras.layers.core.embedding.Embedding object at 0x000001FF6D238B48> True\n",
      "<keras.layers.core.dense.Dense object at 0x000001FFE2487388> True\n",
      "<keras.layers.rnn.bidirectional.Bidirectional object at 0x000001FFE251DFC8> True\n",
      "<keras.layers.merging.concatenate.Concatenate object at 0x000001FFE1DD2D48> True\n",
      "<keras.engine.input_layer.InputLayer object at 0x000001FFE2441888> True\n",
      "<keras.engine.input_layer.InputLayer object at 0x000001FFE27B9548> True\n",
      "<keras.layers.core.dense.Dense object at 0x000001FFE2509388> True\n",
      "<keras.layers.core.dense.Dense object at 0x000001FFE2140148> True\n",
      "<keras.layers.core.dense.Dense object at 0x000001FFE25C5308> True\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x000001FFE1DD7FC8> True\n",
      "<keras.layers.merging.concatenate.Concatenate object at 0x000001FFE1DDA5C8> True\n",
      "<keras.layers.core.dense.Dense object at 0x000001FFE287EAC8> True\n"
     ]
    }
   ],
   "source": [
    "# Check which layers of our model are trainable or not\n",
    "for layer in model_5.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec961d",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------\n",
    "Our model is constructed, let is compile it.\n",
    "\n",
    "We have going to introduce a new parameter to our loss function called `label_smoothing`. Label smoothing helps to regularize our model (orevent overfitting) by making sure it does not get to focused on applying one particular label to a sample.\n",
    "\n",
    "For example, instead of having an output prediction of:\n",
    "    \n",
    "    -`[0.0, 0.0, 1.0, 0.0, 0.0]` for a sample (the model is very confident the right label is index 2).\n",
    "    \n",
    "    It's predictions will get smoothed to be somehting like:\n",
    "    \n",
    "    -`[0.01, 0.01, 0.096, 0.01, 0.01]` giving a small activation to each of the other labels, in turn, hopefully improving generalization.\n",
    "    \n",
    "    For more on label smoothing, greate blog post by PyImageSearch, \n",
    "    (https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0b1c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token, char, positional embedding model\n",
    "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # add label smoothing\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b291b",
   "metadata": {},
   "source": [
    "### Create tribrid embedding datasets and fit  tribrid model\n",
    "\n",
    "Model compiled!\n",
    "\n",
    "Again, to keep our experiments swift, let's fit on 20,000 examples for 3 epochs.\n",
    "\n",
    "This time our model requires four feature inputs:\n",
    "    \n",
    "    1. Train line numbers one-hot tensor (`train_line_numbers_one_hot`)\n",
    "    2. Train total lines one-hot tensor (`train_total_lines_one_hot`)\n",
    "    3. Token-level sequences tensor (`train_sentences`)\n",
    "    4. Char-level sequences tensor (`train_chars`)\n",
    "    \n",
    "We can pass these as tuples to our `tf.data.Dataset.from_tensor_slices()` method to create appropriately shaped and batch `PrefetchedDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d803e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>,\n",
       " <PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation datasets (all four kind of inputs)\n",
    "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                               train_total_lines_one_hot,\n",
    "                                                               train_sentences,\n",
    "                                                               train_chars))\n",
    "\n",
    "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels))\n",
    "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset\n",
    "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                             val_total_lines_one_hot,\n",
    "                                                             val_sentences,\n",
    "                                                             val_chars))\n",
    "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
    "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Check input shapes\n",
    "train_pos_char_token_dataset, val_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2766c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the token, char and positional embedding model\n",
    "history_model_5 = model_5.fit(train_pos_char_token_dataset,\n",
    "                             steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
    "                             epochs=3,\n",
    "                             validation_data=val_pos_char_token_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_pos_char_token_dataset)),\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e424ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 22s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with token-char-positional hybrid model\n",
    "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
    "# Turn prediction probabilities into prediction classes\n",
    "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
    "model_5_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_5_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4597953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 83.43704488282802,\n",
       " 'precision': 0.8336433218866851,\n",
       " 'recall': 0.8343704488282802,\n",
       " 'f1': 0.8331303409352201}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad627a",
   "metadata": {},
   "source": [
    "Compare model results\n",
    "\n",
    "Far out, we have come a long way. From a baseline model to training a model containing three different kinds of embeddings. \n",
    "\n",
    "Now it is time to compare each model's performance against each other.\n",
    "\n",
    "We will also able to compare our model's to PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper.\n",
    "\n",
    "Since all of our model results are in dictionaries, let's combine them into a pandas DataFrame to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93edd75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>72.183238</td>\n",
       "      <td>0.718647</td>\n",
       "      <td>0.721832</td>\n",
       "      <td>0.698925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_token_embed_conv1d</th>\n",
       "      <td>78.674037</td>\n",
       "      <td>0.783339</td>\n",
       "      <td>0.786740</td>\n",
       "      <td>0.784356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_token_embed</th>\n",
       "      <td>71.378922</td>\n",
       "      <td>0.714447</td>\n",
       "      <td>0.713789</td>\n",
       "      <td>0.710714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_char_embed_conv1d</th>\n",
       "      <td>65.232358</td>\n",
       "      <td>0.643353</td>\n",
       "      <td>0.652324</td>\n",
       "      <td>0.642567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid_char_token_embed</th>\n",
       "      <td>69.766980</td>\n",
       "      <td>0.709059</td>\n",
       "      <td>0.697670</td>\n",
       "      <td>0.696851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tribrid_pos_char_token_embed</th>\n",
       "      <td>83.437045</td>\n",
       "      <td>0.833643</td>\n",
       "      <td>0.834370</td>\n",
       "      <td>0.833130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               accuracy  precision    recall        f1\n",
       "baseline                      72.183238   0.718647  0.721832  0.698925\n",
       "custom_token_embed_conv1d     78.674037   0.783339  0.786740  0.784356\n",
       "pretrained_token_embed        71.378922   0.714447  0.713789  0.710714\n",
       "custom_char_embed_conv1d      65.232358   0.643353  0.652324  0.642567\n",
       "hybrid_char_token_embed       69.766980   0.709059  0.697670  0.696851\n",
       "tribrid_pos_char_token_embed  83.437045   0.833643  0.834370  0.833130"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine model results into a DataFrame\n",
    "all_model_results = pd.DataFrame({\"baseline\": baseline_result,\n",
    "                                  \"custom_token_embed_conv1d\": model_1_results,\n",
    "                                  \"pretrained_token_embed\": model_2_results,\n",
    "                                  \"custom_char_embed_conv1d\": model_3_results,\n",
    "                                  \"hybrid_char_token_embed\": model_4_results,\n",
    "                                  \"tribrid_pos_char_token_embed\": model_5_results})\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d8f83f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reduce the accuracy to same scale as other metrics\n",
    "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100\n",
    "# Plot and compaare all of the model results\n",
    "all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0591e3e",
   "metadata": {},
   "source": [
    "Based on F1-scores, it looks like our tribrid embedding model performs the best by a fair margin.\n",
    "\n",
    "Though, in comparison to the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, our model's F1-score is still underperforming (the authors model achieves an F1-score of 90.0 on the 20k RCT dataset versus our F1-score of ~82.6).\n",
    "\n",
    "There are some things to note about this difference:\n",
    "\n",
    "    - Our models (with an exception for the baseline) have been trained on ~18,000 (10% of batches) samples of sequences and labels rather than the full ~18,000 in the 20k RCT dataset.\n",
    "        - This is often the case in machine learning experiments though, make sure trainning works on a smaller number of samples, then upscale when needed (an extension to this project will be training a model on the full dataset).\n",
    "    - Our model's prediction performance levels have been evaluated on the validation dataset not the test databse (we will evaluate our best model on the test dataset shortly).\n",
    "    \n",
    "### Save and load best performing model\n",
    "\n",
    "Since we have been through a fair few experiments, it is a good idea to save our best performing model so we can reuse it without having to retrain it.\n",
    "\n",
    "We can save our best performing model by calling the `save()` method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "24aebc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: skimlit_tribrid_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: skimlit_tribrid_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save best performing model to SavedModel format (default)\n",
    "model_5.save(\"skimlit_tribrid_model\") # model will be saved to path specified by string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd06287",
   "metadata": {},
   "source": [
    "Like all good cooking shows, we have got a pretrained model (exactly the same kind of model we built for `model_5` save and stored on Google Storage)\n",
    "\n",
    "So to make sure we are all using the same model for evaluation, we will download it and load it in.\n",
    "\n",
    "And when loading in our model, since it uses a couple of custom object(our TensorFlow Hub layer and `TextVectorization` layer), we will have to load it in by specifying them in the `custom_objects` parameter of `tf.keras.models.load_model()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037c19e",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "### Make predictions and evalaute them against the truth labels\n",
    "\n",
    "To make sure our model saved and loaded correctly, let's make predictions with it, evaluate them and then compare them to the prediction results we calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "277eb0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 20s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 3, 2, 2, 4, 4, 4, 4, 1], dtype=int64)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with the loaded model on the validation set\n",
    "loaded_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
    "loaded_preds = tf.argmax(loaded_pred_probs, axis=1)\n",
    "loaded_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e0b72763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 83.43704488282802,\n",
       " 'precision': 0.8336433218866851,\n",
       " 'recall': 0.8343704488282802,\n",
       " 'f1': 0.8331303409352201}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate loaded model's predictions\n",
    "loaded_model_results = calculate_results(val_labels_encoded,\n",
    "                                         loaded_preds)\n",
    "loaded_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e646d4",
   "metadata": {},
   "source": [
    "Let's compare our loaded model's predictions with the prediction results we obtained before saving our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "412f7dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare loaded model results with original trained model results (should be quite close)\n",
    "np.isclose(list(model_5_results.values()), list(loaded_model_results.values()), rtol=1e-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4a443",
   "metadata": {},
   "source": [
    "It's worth noting that loading in a SavedModel unfreezes all layers (make them all trainable). So if you want to freeze any layers, you will have to set their trainable attribute to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be151812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_inputs (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_inputs (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVectoriza  (None, 290)         0           ['char_inputs[0][0]']            \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_inputs[0][0]']           \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 290, 25)      1750        ['char_vectorizer[3][0]']        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[3][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 64)          14848       ['char_embed[3][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid_embedding (C  (None, 192)         0           ['dense_8[0][0]',                \n",
      " oncatenate)                                                      'bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " line_number_input (InputLayer)  [(None, 15)]        0           []                               \n",
      "                                                                                                  \n",
      " total_lines_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 256)          49408       ['token_char_hybrid_embedding[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 32)           512         ['line_number_input[0][0]']      \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           672         ['total_lines_input[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " token_char_positional_embeddin  (None, 320)         0           ['dense_9[0][0]',                \n",
      " g (Concatenate)                                                  'dense_10[0][0]',               \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 5)            1605        ['token_char_positional_embedding\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,932,283\n",
      "Trainable params: 134,459\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check loaded model summary (note the number of trainable parameters)\n",
    "loaded_model = model_5\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f502a6",
   "metadata": {},
   "source": [
    "Evaluate model on test dataset\n",
    "\n",
    "To make our model's performance more comparable with the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, let's make predictions on the test dataset and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d292319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test dataset batch and prefetched\n",
    "test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,\n",
    "                                                              test_total_lines_one_hot,\n",
    "                                                              test_sentences,\n",
    "                                                              test_chars))\n",
    "test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels))\n",
    "test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Check shapes\n",
    "test_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "68b2f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 25s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 0, 2, 2, 4, 4, 4, 1, 1, 0], dtype=int64)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the test dataset\n",
    "test_pred_probs = loaded_model.predict(test_pos_char_token_dataset,\n",
    "                                      verbose=1)\n",
    "test_preds = tf.argmax(test_pred_probs, axis=1)\n",
    "test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "31ca3166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 82.96333167413307,\n",
       " 'precision': 0.8283806746632603,\n",
       " 'recall': 0.8296333167413307,\n",
       " 'f1': 0.828287097671159}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate loaded model test predictions\n",
    "loaded_model_test_results = calculate_results(y_true=test_labels_encoded,\n",
    "                                             y_pred=test_preds)\n",
    "loaded_model_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e5eb0",
   "metadata": {},
   "source": [
    "It seems our best model (so far) still has some ways to go to match the performance of the results in the paper (their model gets 90.1 F1-score on the test dataset, where as ours gets 82.1 Fi-score).\n",
    "\n",
    "However, as we discussed before our model has only been trained on 20,000 out of the total ~180,000 sequences in the RCT 20k dataset. We also haven't fine-tuned our pretrained embeddings (the paper fine-tunes GloVe embeddings). So there is a couple of extensions we could try to improve our results.\n",
    "\n",
    "### Find most wrong\n",
    "\n",
    "One of the best ways to investigate where your model is going wrong (or potentially where your data is wrong) is to visualize the \"most wrong\" predictions.\n",
    "\n",
    "The most wrong predictions are samples where the model has made a prediction with a high probability but has gotten it wrong (the model's prediction disagreess with the ground truth label).\n",
    "\n",
    "Looking at the most wrong predictions can give us valuable information on how to improve further models or fix the labels in our data.\n",
    "\n",
    "Let's write some code to help is visualize the most wrong predictions from the test dataset.\n",
    "\n",
    "Frist we will convert all of our integer-based test predictions into their string-based class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "581b80b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get list of class names of test predictions\n",
    "test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffda116",
   "metadata": {},
   "source": [
    "Now we will enrich our test DataFrame with a few values:\n",
    "\n",
    "    - A `\"prediction\"` (string) column containing our model's prediction for a given sample.\n",
    "    - A `\"pred_prob\"` (float) column containing the model's maximum prediction probability for a given sample.\n",
    "    - A `\"correct\"` (bool) column to indicate whether or not the model's prediction matches the sample's target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "90f4bc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>this study analyzed liver function abnormaliti...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.540891</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>a post hoc analysis was conducted with the use...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>0.340717</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>liver function tests ( lfts ) were measured at...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.799080</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>survival analyses were used to assess the asso...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.648001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the percentage of patients with abnormal lfts ...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.732175</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               text  line_number  \\\n",
       "0  BACKGROUND  this study analyzed liver function abnormaliti...            0   \n",
       "1     RESULTS  a post hoc analysis was conducted with the use...            1   \n",
       "2     RESULTS  liver function tests ( lfts ) were measured at...            2   \n",
       "3     RESULTS  survival analyses were used to assess the asso...            3   \n",
       "4     RESULTS  the percentage of patients with abnormal lfts ...            4   \n",
       "\n",
       "   total_lines  prediction  pred_prob  correct  \n",
       "0            8   OBJECTIVE   0.540891    False  \n",
       "1            8  BACKGROUND   0.340717    False  \n",
       "2            8     METHODS   0.799080    False  \n",
       "3            8     METHODS   0.648001    False  \n",
       "4            8     RESULTS   0.732175     True  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create production-enriched test dataframe\n",
    "test_df[\"prediction\"] = test_pred_classes\n",
    "test_df[\"pred_prob\"] = tf.reduce_max(test_pred_probs, axis=1).numpy()\n",
    "test_df[\"correct\"] = test_df[\"prediction\"] == test_df[\"target\"]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef283c",
   "metadata": {},
   "source": [
    "Having our data like this, makes it very easy to manipulate and view in different ways.\n",
    "\n",
    "How about we sort our DataFrame to find the samples with the highest \"pred_prod\" and where the prediction was wrong \"correct\" == False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e11c005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the primary endpoint is the cumulative three-y...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13874</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>symptom outcomes will be assessed and estimate...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.935220</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pretest-posttest .</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>0.929654</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16347</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>to evaluate the effects of the lactic acid bac...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.925114</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>a screening questionnaire for moh was sent to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.919426</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                                               text  \\\n",
       "2388       RESULTS  the primary endpoint is the cumulative three-y...   \n",
       "13874  CONCLUSIONS  symptom outcomes will be assessed and estimate...   \n",
       "8545       METHODS                                 pretest-posttest .   \n",
       "16347   BACKGROUND  to evaluate the effects of the lactic acid bac...   \n",
       "697        RESULTS  a screening questionnaire for moh was sent to ...   \n",
       "\n",
       "       line_number  total_lines  prediction  pred_prob  correct  \n",
       "2388             4           13     METHODS   0.936370    False  \n",
       "13874            4            6     METHODS   0.935220    False  \n",
       "8545             1           11  BACKGROUND   0.929654    False  \n",
       "16347            0           12   OBJECTIVE   0.925114    False  \n",
       "697              4           14     METHODS   0.919426    False  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find top 100 most wrong samples (note: 100 is an abitrary number, you could go through all of them if you wanted)\n",
    "top_100_wrong = test_df[test_df[\"correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100]\n",
    "top_100_wrong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f2567",
   "metadata": {},
   "source": [
    "We got a subset of our model's most wrong predictions, let's write some code to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "93fc2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: RESULTS, Pred: METHODS, Prob: 0.9363704919815063, Line number: 4, Total lines: 13\n",
      "\n",
      "Text:\n",
      "the primary endpoint is the cumulative three-year hiv incidence .\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate top wrong preds\n",
    "for row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples\n",
    "    _, target, text, line_number, total_lines, prediction, pred_prob, _ = row\n",
    "    print(f\"Target: {target}, Pred: {prediction}, Prob: {pred_prob}, Line number: {line_number}, Total lines: {total_lines}\\n\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"-----\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21337c",
   "metadata": {},
   "source": [
    "What do you notice about the most wrong predictions? Dose the model make silly mistakes? or are some of the labels incorrect/ambiguous (e.g a line in an abstract could potentially be labelled `OBJECTIVE` or `BACKGROUND` and make sense).\n",
    "\n",
    "A next step here world be if there are a fair few samples with inconsistent labels, you could go through your trainning dataset, update the labels and then retrain a model. The process of using a model to help improve/investigate your dataset's labels is update the labels and then retrain a model. The process of using a model to help improve/investigate your dataset's labels is often referred to as active learning.\n",
    "\n",
    "### Make example predictions\n",
    "\n",
    "We have made some predictions on the test dataset, now's time to really test our model out.\n",
    "\n",
    "To do so, we are going to get some data from the wild and see how our model performs. \n",
    "\n",
    "In other words, were going to find an RCT abstract from PubMed, preprocess the text so it works with our model, then pass each sequence in the wild abstract through our model to see what label it predicts.\n",
    "\n",
    "For an appropriate sample, we will need to search PubMed for RCT's (randomized controlled trials) without abstracts which have been split up (on exploring PubMed you will notice many of the basracts are already preformatted into separate sections, this helps dramatically with readability).\n",
    "\n",
    "Going through various PubMed studies, I managed to find the following unstructured abstract from `RCT of a manualized social treatment for high-functioning autism spectrum disorders`:\n",
    "\n",
    "This RCT examined the effcacy of a manualized social intervention for children with HFASDs. Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition. Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures). Secondary meansures based on staff ratings (treatment group only) corroborated gains reported by parents. Hign levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity. Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.\n",
    "\n",
    "Looking at the large chunk of text can seem quite intimidating. Now imagine you are a medical researcher trying to skim through the literature to find a study relevant to your work.\n",
    "\n",
    "Sources like quite the challenge right?\n",
    "\n",
    "Enter SkimLit!\n",
    "\n",
    "Let's see what our best model so far (`model_5`) makes of the above abstract.\n",
    "\n",
    "But wait...\n",
    "\n",
    "As you might have guessed the above abstract hasnot been formatted in the same structure as the data our model has been trained on. Therefore, before we can make a prediction on it, we need to preprocess it just as we have our other sequences.\n",
    "\n",
    "More specifically, for each abstract, we will need to: \n",
    "\n",
    "    1. Split it into sentences (lines).\n",
    "    2. Split it into characters.\n",
    "    3. Find the number of each line.\n",
    "    4. Find the total number of lines.\n",
    "    \n",
    "Starting with number 1, there are a couple of ways to split our  abstracts into actual sentences. A simple one would be to use Python's in-build `split()` string method, splitting the abstract wherever a fullstop appears. However, can you imagine where this might go wrong?\n",
    "\n",
    "Another more advanced option would be to leverage `spaCy's` (a very powerful NLP library) `sentencizer` class. Which is an easy to use sentence splitter based on spaCy's English language model.\n",
    "\n",
    "I have prepared some abstracts from PubMed RCT papers to try our model on, we can download them `from GitHub`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9526480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728449d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd7f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a02be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662e9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e9f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513be9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b430a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f476e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647feb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2389df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651cb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97216a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cf357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb486c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ff360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39f567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194e4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deaa542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

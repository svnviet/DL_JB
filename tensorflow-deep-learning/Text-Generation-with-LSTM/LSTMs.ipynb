{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "folder, dirs, filenames = next(os.walk(data_dir))\n",
    "filenames = [os.path.join(data_dir, name) for name in filenames] \n",
    "filenames[:5], len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3433d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_size(filenames): \n",
    "    for filename in filenames:\n",
    "        doc = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for row in f:\n",
    "                doc.append(row.lower())\n",
    "            doc = \" \".join(doc).split()\n",
    "    return doc\n",
    "n_vocab = len(set(get_vocabulary_size(train_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae622e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_window_dataset(filenames, window_size=10, shuffle=False, batch_size=32):\n",
    "    documents = []\n",
    "    for f in filenames:\n",
    "        doc = tf.io.read_file(f)\n",
    "        doc = tf.strings.lower(doc) # convert to lower case\n",
    "        doc = tf.strings.regex_replace(doc,\"\\n\",\" \") # replacing new line with space\n",
    "        doc = tf.strings.regex_replace(doc, \"\\r\", \"\") # replcaing \\r charter\n",
    "        doc = tf.strings.split(doc, ' ') # split to word level \n",
    "        documents = documents + doc.numpy().tolist() # make dict word all file\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents) # load data into tensor from\n",
    "    \n",
    "    # make generate windows from list documents \n",
    "    # E.x. ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "    doc_dataset = doc_dataset.window(size=window_size + 1, shift=1, drop_remainder=True)\n",
    "    \n",
    "    # E.x. [['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
    "    doc_dataset = doc_dataset.flat_map(lambda x: x.batch(window_size + 1))\n",
    "    \n",
    "    # E.x. ['A', 'B', 'C', 'D', 'E', 'F'] => [G]    \n",
    "    #      ['B', 'C', 'D', 'E', 'F', 'G'] => [K]\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1] , x[-1:]))\n",
    "    \n",
    "    # shuffle the dataset with window size\n",
    "    if shuffle:\n",
    "        doc_dataset = doc_dataset.shuffle(len(list(doc_dataset)))\n",
    "    \n",
    "    doc_dataset = doc_dataset.batch(batch_size)\n",
    "    doc_dataset = doc_dataset.prefetch(batch_size)\n",
    "    \n",
    "    return doc_dataset , documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb5b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b16b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cbb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d730dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ea896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f589150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba37d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740972ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65255c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936322b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8839d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f711a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb8112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5159793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05591bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f936f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682b1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2f209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8ba55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc3e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe415b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0bfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976cd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440fb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cce88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

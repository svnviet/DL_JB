{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1c6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "import keras.utils as utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b73564a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['data\\\\001.txt',\n",
       "  'data\\\\002.txt',\n",
       "  'data\\\\003.txt',\n",
       "  'data\\\\004.txt',\n",
       "  'data\\\\005.txt'],\n",
       " 209)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "folder, dirs, filenames = next(os.walk(data_dir))\n",
    "filenames = [os.path.join(data_dir, name) for name in filenames] \n",
    "filenames[:5], len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c6bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_size(filenames): \n",
    "    doc = []\n",
    "    for filename in filenames:\n",
    "        n_vocab = 0\n",
    "        with open(filename, 'r') as f:\n",
    "            for row in f:\n",
    "                if len(row) == 0:\n",
    "                    continue\n",
    "                row = row.lower().replace(\"\\r\",\"\").replace(\"\\n\",\" \").replace(\"/[^a-zA-z0-9 ]/g\", \"\")\n",
    "                row = row.replace(\"'\", \"\").replace(\"-\",\"\")\n",
    "                doc.append(row)\n",
    "\n",
    "    return doc\n",
    "all_text = get_vocabulary_size(filenames)\n",
    "all_text = ' '.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d42d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,\n",
       " 'in olden times when wishing still helped one, there lived a king  whose daughters were all beautiful, but the youngest was so beautiful  that the sun itself, which has seen so much, was astonished whenever  it shone in her face.  close by the kings castle lay a great dark  forest, and under an old limetree in the forest was a well, and when  the day was very warm, the kings child went out into the forest and  sat down by the side of the cool fountain, and when she was bored she  took a golden ball, and threw it up on high and caught it, and this  ball was her favorite plaything.    now it so happened that on one occasion the princesss golden ball  did not fall into the little hand which she was holding up for it,  but on to the ground beyond, and rolled straight into the water.  the  kings daughter followed it with her eyes, but it vanished, and the  well was deep, so deep that the bottom could not be seen.  at this  she began to cry, and cried louder and louder, and could not be  comf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocal = len(set(all_text))\n",
    "n_vocal, all_text[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d6ca6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'olden',\n",
       " 'times',\n",
       " 'when',\n",
       " 'wishing',\n",
       " 'still',\n",
       " 'helped',\n",
       " 'one,',\n",
       " 'there',\n",
       " 'lived']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = all_text.split()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c743e859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8032, [[10], [2728], [692], [22], [1413], [114], [933], [35], [37], [292]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    return total_words\n",
    "total_words = get_sequence_of_tokens(corpus)\n",
    "total_words, tokenizer.texts_to_sequences(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eda3b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = utils.to_categorical([[1], [2], [3], [4], [5]], num_classes=11)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd64af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10], [2728]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['in', 'olden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7899a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tokenizer(doc):\n",
    "    return tokenizer.texts_to_sequences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfc269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "def generate_window_dataset(documents, window_size=10, shuffle=True, batch_size=32):\n",
    "    \n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents) # load data into tensor from\n",
    "    \n",
    "#     # make generate windows from list documents \n",
    "#     # E.x. ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "    doc_dataset = doc_dataset.window(size=window_size + 1, shift=1, drop_remainder=True)\n",
    "    \n",
    "# #     # E.x. [['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
    "    doc_dataset = doc_dataset.flat_map(lambda x: x.batch(window_size + 1))\n",
    "    \n",
    "# #     # E.x. ['A', 'B', 'C', 'D', 'E', 'F'] => [G]    \n",
    "# #     #      ['B', 'C', 'D', 'E', 'F', 'G'] => [K]\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1] , x[-1:]))\n",
    "    \n",
    "#     # shuffle the dataset with window size\n",
    "    if shuffle:\n",
    "        doc_dataset = doc_dataset.shuffle(len(list(doc_dataset)))\n",
    "    \n",
    "    doc_dataset = doc_dataset.batch(batch_size)\n",
    "    doc_dataset = doc_dataset.prefetch(batch_size)\n",
    "    \n",
    "    return doc_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c65fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_window_dataset(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27a4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7fd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizers = [x[0] for x in tokenizer.texts_to_sequences(corpus) if len(x) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b3cedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 2728,\n",
       " 692,\n",
       " 22,\n",
       " 1413,\n",
       " 114,\n",
       " 933,\n",
       " 35,\n",
       " 37,\n",
       " 292,\n",
       " 5,\n",
       " 55,\n",
       " 590,\n",
       " 659,\n",
       " 45,\n",
       " 38,\n",
       " 146,\n",
       " 18,\n",
       " 1,\n",
       " 450,\n",
       " 6,\n",
       " 30,\n",
       " 146,\n",
       " 13,\n",
       " 1,\n",
       " 339,\n",
       " 538,\n",
       " 49,\n",
       " 104,\n",
       " 274,\n",
       " 30,\n",
       " 150,\n",
       " 6,\n",
       " 858,\n",
       " 1053,\n",
       " 7,\n",
       " 672,\n",
       " 10,\n",
       " 16,\n",
       " 313,\n",
       " 746,\n",
       " 54,\n",
       " 1,\n",
       " 105,\n",
       " 191,\n",
       " 166,\n",
       " 5,\n",
       " 81,\n",
       " 557,\n",
       " 120,\n",
       " 2,\n",
       " 268,\n",
       " 92,\n",
       " 73,\n",
       " 2094,\n",
       " 10,\n",
       " 1,\n",
       " 120,\n",
       " 6,\n",
       " 5,\n",
       " 82,\n",
       " 2,\n",
       " 22,\n",
       " 1,\n",
       " 112,\n",
       " 6,\n",
       " 141,\n",
       " 693,\n",
       " 1,\n",
       " 105,\n",
       " 145,\n",
       " 39,\n",
       " 34,\n",
       " 36,\n",
       " 1,\n",
       " 120,\n",
       " 2,\n",
       " 170,\n",
       " 58,\n",
       " 54,\n",
       " 1,\n",
       " 340,\n",
       " 8,\n",
       " 1,\n",
       " 1961,\n",
       " 2095,\n",
       " 2,\n",
       " 22,\n",
       " 12,\n",
       " 6,\n",
       " 3552,\n",
       " 12,\n",
       " 67,\n",
       " 5,\n",
       " 175,\n",
       " 985,\n",
       " 2,\n",
       " 304,\n",
       " 7,\n",
       " 51,\n",
       " 26,\n",
       " 400,\n",
       " 2,\n",
       " 494,\n",
       " 7,\n",
       " 2,\n",
       " 56,\n",
       " 985,\n",
       " 6,\n",
       " 16,\n",
       " 2729,\n",
       " 3553,\n",
       " 68,\n",
       " 7,\n",
       " 30,\n",
       " 280,\n",
       " 13,\n",
       " 26,\n",
       " 35,\n",
       " 2464,\n",
       " 1,\n",
       " 3086,\n",
       " 175,\n",
       " 985,\n",
       " 64,\n",
       " 21,\n",
       " 368,\n",
       " 36,\n",
       " 1,\n",
       " 50,\n",
       " 189,\n",
       " 49,\n",
       " 12,\n",
       " 6,\n",
       " 1962,\n",
       " 51,\n",
       " 25,\n",
       " 7,\n",
       " 18,\n",
       " 26,\n",
       " 3,\n",
       " 1,\n",
       " 266,\n",
       " 2262,\n",
       " 2,\n",
       " 777,\n",
       " 525,\n",
       " 36,\n",
       " 1,\n",
       " 147,\n",
       " 1,\n",
       " 105,\n",
       " 94,\n",
       " 572,\n",
       " 7,\n",
       " 20,\n",
       " 16,\n",
       " 179,\n",
       " 18,\n",
       " 7,\n",
       " 859,\n",
       " 2,\n",
       " 1,\n",
       " 82,\n",
       " 6,\n",
       " 602,\n",
       " 30,\n",
       " 602,\n",
       " 13,\n",
       " 1,\n",
       " 760,\n",
       " 65,\n",
       " 21,\n",
       " 32,\n",
       " 274,\n",
       " 31,\n",
       " 56,\n",
       " 12,\n",
       " 144,\n",
       " 3,\n",
       " 673,\n",
       " 2,\n",
       " 100,\n",
       " 2263,\n",
       " 2,\n",
       " 2263,\n",
       " 2,\n",
       " 65,\n",
       " 21,\n",
       " 32,\n",
       " 960,\n",
       " 2,\n",
       " 27,\n",
       " 12,\n",
       " 310,\n",
       " 1492,\n",
       " 645,\n",
       " 14,\n",
       " 3,\n",
       " 16,\n",
       " 48,\n",
       " 2264,\n",
       " 9,\n",
       " 105,\n",
       " 94,\n",
       " 9,\n",
       " 706,\n",
       " 30,\n",
       " 13,\n",
       " 301,\n",
       " 5,\n",
       " 320,\n",
       " 62,\n",
       " 558,\n",
       " 674,\n",
       " 12,\n",
       " 143,\n",
       " 211,\n",
       " 3,\n",
       " 1,\n",
       " 340,\n",
       " 60,\n",
       " 761,\n",
       " 1,\n",
       " 429,\n",
       " 47,\n",
       " 2,\n",
       " 74,\n",
       " 5,\n",
       " 887,\n",
       " 4188,\n",
       " 238,\n",
       " 219,\n",
       " 481,\n",
       " 1054,\n",
       " 153,\n",
       " 60,\n",
       " 1,\n",
       " 147,\n",
       " 230,\n",
       " 73,\n",
       " 5276,\n",
       " 33,\n",
       " 7,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 11,\n",
       " 97,\n",
       " 860,\n",
       " 25,\n",
       " 42,\n",
       " 175,\n",
       " 985,\n",
       " 49,\n",
       " 104,\n",
       " 616,\n",
       " 36,\n",
       " 1,\n",
       " 82,\n",
       " 32,\n",
       " 646,\n",
       " 2,\n",
       " 53,\n",
       " 21,\n",
       " 706,\n",
       " 103,\n",
       " 1,\n",
       " 887,\n",
       " 11,\n",
       " 87,\n",
       " 261,\n",
       " 9,\n",
       " 18,\n",
       " 48,\n",
       " 29,\n",
       " 9,\n",
       " 106,\n",
       " 40,\n",
       " 41,\n",
       " 11,\n",
       " 246,\n",
       " 61,\n",
       " 3553,\n",
       " 51,\n",
       " 52,\n",
       " 1020,\n",
       " 9,\n",
       " 29,\n",
       " 28,\n",
       " 181,\n",
       " 887,\n",
       " 14,\n",
       " 12,\n",
       " 42,\n",
       " 482,\n",
       " 42,\n",
       " 1055,\n",
       " 2,\n",
       " 1021,\n",
       " 2,\n",
       " 301,\n",
       " 1,\n",
       " 175,\n",
       " 1022,\n",
       " 49,\n",
       " 11,\n",
       " 97,\n",
       " 2730,\n",
       " 1,\n",
       " 887,\n",
       " 103,\n",
       " 11,\n",
       " 53,\n",
       " 21,\n",
       " 413,\n",
       " 25,\n",
       " 61,\n",
       " 482,\n",
       " 61,\n",
       " 1055,\n",
       " 2,\n",
       " 1021,\n",
       " 565,\n",
       " 25,\n",
       " 61,\n",
       " 175,\n",
       " 1022,\n",
       " 18,\n",
       " 41,\n",
       " 9,\n",
       " 29,\n",
       " 519,\n",
       " 40,\n",
       " 2,\n",
       " 99,\n",
       " 40,\n",
       " 32,\n",
       " 61,\n",
       " 1493,\n",
       " 2,\n",
       " 5277,\n",
       " 2,\n",
       " 465,\n",
       " 54,\n",
       " 9,\n",
       " 31,\n",
       " 61,\n",
       " 50,\n",
       " 233,\n",
       " 2,\n",
       " 184,\n",
       " 107,\n",
       " 61,\n",
       " 50,\n",
       " 175,\n",
       " 1206,\n",
       " 2,\n",
       " 327,\n",
       " 34,\n",
       " 8,\n",
       " 61,\n",
       " 50,\n",
       " 1414,\n",
       " 2,\n",
       " 269,\n",
       " 10,\n",
       " 61,\n",
       " 50,\n",
       " 193,\n",
       " 41,\n",
       " 9,\n",
       " 29,\n",
       " 512,\n",
       " 40,\n",
       " 56,\n",
       " 11,\n",
       " 29,\n",
       " 57,\n",
       " 58,\n",
       " 582,\n",
       " 2,\n",
       " 246,\n",
       " 9,\n",
       " 61,\n",
       " 175,\n",
       " 985,\n",
       " 51,\n",
       " 52,\n",
       " 209,\n",
       " 225,\n",
       " 14,\n",
       " 12,\n",
       " 11,\n",
       " 512,\n",
       " 9,\n",
       " 38,\n",
       " 9,\n",
       " 316,\n",
       " 41,\n",
       " 9,\n",
       " 29,\n",
       " 18,\n",
       " 246,\n",
       " 40,\n",
       " 42,\n",
       " 985,\n",
       " 86,\n",
       " 52,\n",
       " 18,\n",
       " 12,\n",
       " 89,\n",
       " 79,\n",
       " 1,\n",
       " 1494,\n",
       " 887,\n",
       " 330,\n",
       " 1649,\n",
       " 38,\n",
       " 4,\n",
       " 330,\n",
       " 33,\n",
       " 3,\n",
       " 465,\n",
       " 10,\n",
       " 1,\n",
       " 147,\n",
       " 20,\n",
       " 1,\n",
       " 113,\n",
       " 1353,\n",
       " 2,\n",
       " 2731,\n",
       " 4,\n",
       " 87,\n",
       " 32,\n",
       " 46,\n",
       " 1493,\n",
       " 3,\n",
       " 192,\n",
       " 566,\n",
       " 444,\n",
       " 18,\n",
       " 1,\n",
       " 887,\n",
       " 22,\n",
       " 4,\n",
       " 17,\n",
       " 455,\n",
       " 56,\n",
       " 512,\n",
       " 111,\n",
       " 15,\n",
       " 153,\n",
       " 36,\n",
       " 1,\n",
       " 147,\n",
       " 2,\n",
       " 1123,\n",
       " 58,\n",
       " 2,\n",
       " 10,\n",
       " 5,\n",
       " 545,\n",
       " 270,\n",
       " 47,\n",
       " 5278,\n",
       " 51,\n",
       " 52,\n",
       " 20,\n",
       " 1,\n",
       " 985,\n",
       " 10,\n",
       " 15,\n",
       " 487,\n",
       " 2,\n",
       " 304,\n",
       " 7,\n",
       " 26,\n",
       " 1,\n",
       " 910,\n",
       " 1,\n",
       " 105,\n",
       " 94,\n",
       " 6,\n",
       " 675,\n",
       " 3,\n",
       " 115,\n",
       " 16,\n",
       " 435,\n",
       " 3553,\n",
       " 70,\n",
       " 85,\n",
       " 2,\n",
       " 747,\n",
       " 7,\n",
       " 51,\n",
       " 2,\n",
       " 173,\n",
       " 63,\n",
       " 20,\n",
       " 7,\n",
       " 401,\n",
       " 401,\n",
       " 14,\n",
       " 1,\n",
       " 887,\n",
       " 101,\n",
       " 40,\n",
       " 20,\n",
       " 9,\n",
       " 11,\n",
       " 934,\n",
       " 296,\n",
       " 27,\n",
       " 9,\n",
       " 87,\n",
       " 18,\n",
       " 48,\n",
       " 64,\n",
       " 7,\n",
       " 2732,\n",
       " 19,\n",
       " 3,\n",
       " 1842,\n",
       " 15,\n",
       " 2731,\n",
       " 2731,\n",
       " 118,\n",
       " 16,\n",
       " 27,\n",
       " 832,\n",
       " 27,\n",
       " 4,\n",
       " 65,\n",
       " 12,\n",
       " 64,\n",
       " 21,\n",
       " 633,\n",
       " 3,\n",
       " 7,\n",
       " 18,\n",
       " 173,\n",
       " 91,\n",
       " 2,\n",
       " 160,\n",
       " 1296,\n",
       " 1,\n",
       " 158,\n",
       " 887,\n",
       " 44,\n",
       " 6,\n",
       " 324,\n",
       " 3,\n",
       " 57,\n",
       " 86,\n",
       " 36,\n",
       " 15,\n",
       " 82,\n",
       " 52,\n",
       " 1,\n",
       " 195,\n",
       " 112,\n",
       " 22,\n",
       " 12,\n",
       " 17,\n",
       " 385,\n",
       " 159,\n",
       " 31,\n",
       " 233,\n",
       " 20,\n",
       " 1,\n",
       " 55,\n",
       " 2,\n",
       " 38,\n",
       " 1,\n",
       " 3087,\n",
       " 2,\n",
       " 6,\n",
       " 861,\n",
       " 60,\n",
       " 16,\n",
       " 50,\n",
       " 175,\n",
       " 1206,\n",
       " 202,\n",
       " 47,\n",
       " 1650,\n",
       " 4189,\n",
       " 3554,\n",
       " 4189,\n",
       " 3554,\n",
       " 51,\n",
       " 1,\n",
       " 2465,\n",
       " 3555,\n",
       " 2,\n",
       " 22,\n",
       " 7,\n",
       " 17,\n",
       " 126,\n",
       " 3,\n",
       " 1,\n",
       " 539,\n",
       " 7,\n",
       " 495,\n",
       " 31,\n",
       " 1,\n",
       " 136,\n",
       " 2,\n",
       " 100,\n",
       " 344,\n",
       " 450,\n",
       " 344,\n",
       " 278,\n",
       " 1,\n",
       " 136,\n",
       " 25,\n",
       " 40,\n",
       " 12,\n",
       " 173,\n",
       " 3,\n",
       " 115,\n",
       " 44,\n",
       " 6,\n",
       " 379,\n",
       " 18,\n",
       " 22,\n",
       " 12,\n",
       " 241,\n",
       " 1,\n",
       " 136,\n",
       " 37,\n",
       " 170,\n",
       " 1,\n",
       " 887,\n",
       " 10,\n",
       " 451,\n",
       " 8,\n",
       " 7,\n",
       " 23,\n",
       " 12,\n",
       " 5279,\n",
       " 1,\n",
       " 136,\n",
       " 3,\n",
       " 10,\n",
       " 81,\n",
       " 862,\n",
       " 170,\n",
       " 58,\n",
       " 3,\n",
       " 748,\n",
       " 52,\n",
       " 2,\n",
       " 6,\n",
       " 130,\n",
       " 1056,\n",
       " 1,\n",
       " 55,\n",
       " 74,\n",
       " 3088,\n",
       " 13,\n",
       " 16,\n",
       " 187,\n",
       " 6,\n",
       " 1207,\n",
       " 1570,\n",
       " 2,\n",
       " 14,\n",
       " 42,\n",
       " 145,\n",
       " 48,\n",
       " 59,\n",
       " 9,\n",
       " 30,\n",
       " 445,\n",
       " 8,\n",
       " 33,\n",
       " 37,\n",
       " 5280,\n",
       " 5,\n",
       " 333,\n",
       " 379,\n",
       " 44,\n",
       " 863,\n",
       " 3,\n",
       " 281,\n",
       " 9,\n",
       " 63,\n",
       " 230,\n",
       " 46,\n",
       " 214,\n",
       " 12,\n",
       " 7,\n",
       " 33,\n",
       " 46,\n",
       " 333,\n",
       " 18,\n",
       " 5,\n",
       " 4190,\n",
       " 887,\n",
       " 48,\n",
       " 330,\n",
       " 5,\n",
       " 887,\n",
       " 242,\n",
       " 20,\n",
       " 9,\n",
       " 230,\n",
       " 181,\n",
       " 90,\n",
       " 1843,\n",
       " 27,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 1,\n",
       " 120,\n",
       " 207,\n",
       " 54,\n",
       " 1,\n",
       " 82,\n",
       " 1297,\n",
       " 42,\n",
       " 175,\n",
       " 985,\n",
       " 142,\n",
       " 36,\n",
       " 1,\n",
       " 147,\n",
       " 2,\n",
       " 293,\n",
       " 11,\n",
       " 100,\n",
       " 30,\n",
       " 1,\n",
       " 887,\n",
       " 152,\n",
       " 7,\n",
       " 34,\n",
       " 52,\n",
       " 25,\n",
       " 40,\n",
       " 2,\n",
       " 293,\n",
       " 4,\n",
       " 30,\n",
       " 2466,\n",
       " 11,\n",
       " 488,\n",
       " 19,\n",
       " 4,\n",
       " 122,\n",
       " 32,\n",
       " 42,\n",
       " 1493,\n",
       " 18,\n",
       " 11,\n",
       " 168,\n",
       " 89,\n",
       " 4,\n",
       " 62,\n",
       " 32,\n",
       " 389,\n",
       " 3,\n",
       " 72,\n",
       " 34,\n",
       " 8,\n",
       " 15,\n",
       " 147,\n",
       " 2,\n",
       " 68,\n",
       " 4,\n",
       " 33,\n",
       " 379,\n",
       " 37,\n",
       " 2,\n",
       " 863,\n",
       " 3,\n",
       " 72,\n",
       " 10,\n",
       " 3,\n",
       " 40,\n",
       " 10,\n",
       " 1,\n",
       " 634,\n",
       " 7,\n",
       " 495,\n",
       " 5,\n",
       " 247,\n",
       " 75,\n",
       " 2,\n",
       " 100,\n",
       " 344,\n",
       " 450,\n",
       " 344,\n",
       " 278,\n",
       " 1,\n",
       " 136,\n",
       " 25,\n",
       " 40,\n",
       " 53,\n",
       " 9,\n",
       " 21,\n",
       " 140,\n",
       " 48,\n",
       " 9,\n",
       " 14,\n",
       " 3,\n",
       " 40,\n",
       " 1843,\n",
       " 54,\n",
       " 1,\n",
       " 1961,\n",
       " 3556,\n",
       " 8,\n",
       " 1,\n",
       " 82,\n",
       " 344,\n",
       " 450,\n",
       " 344,\n",
       " 278,\n",
       " 1,\n",
       " 136,\n",
       " 25,\n",
       " 40,\n",
       " 23,\n",
       " 14,\n",
       " 1,\n",
       " 55,\n",
       " 13,\n",
       " 49,\n",
       " 9,\n",
       " 28,\n",
       " 488,\n",
       " 76,\n",
       " 9,\n",
       " 1298,\n",
       " 57,\n",
       " 2,\n",
       " 99,\n",
       " 19,\n",
       " 10,\n",
       " 12,\n",
       " 39,\n",
       " 2,\n",
       " 241,\n",
       " 1,\n",
       " 136,\n",
       " 2,\n",
       " 1,\n",
       " 887,\n",
       " 2096,\n",
       " 10,\n",
       " 2,\n",
       " 572,\n",
       " 16,\n",
       " 986,\n",
       " 54,\n",
       " 986,\n",
       " 3,\n",
       " 16,\n",
       " 1023,\n",
       " 37,\n",
       " 4,\n",
       " 170,\n",
       " 2,\n",
       " 100,\n",
       " 1844,\n",
       " 40,\n",
       " 51,\n",
       " 446,\n",
       " 9,\n",
       " 12,\n",
       " 4191,\n",
       " 137,\n",
       " 31,\n",
       " 132,\n",
       " 1,\n",
       " 55,\n",
       " 1057,\n",
       " 16,\n",
       " 3,\n",
       " 53,\n",
       " 7,\n",
       " 70,\n",
       " 1,\n",
       " 887,\n",
       " 6,\n",
       " 26,\n",
       " 1,\n",
       " 1023,\n",
       " 4,\n",
       " 183,\n",
       " 3,\n",
       " 32,\n",
       " 26,\n",
       " 1,\n",
       " 233,\n",
       " 2,\n",
       " 22,\n",
       " 4,\n",
       " 6,\n",
       " 26,\n",
       " 1,\n",
       " 233,\n",
       " 4,\n",
       " 14,\n",
       " 68,\n",
       " 1963,\n",
       " 61,\n",
       " 50,\n",
       " 175,\n",
       " 1206,\n",
       " 1168,\n",
       " 3,\n",
       " 40,\n",
       " 13,\n",
       " 80,\n",
       " 185,\n",
       " 184,\n",
       " 148,\n",
       " 12,\n",
       " 64,\n",
       " 56,\n",
       " 18,\n",
       " 7,\n",
       " 6,\n",
       " 888,\n",
       " 3,\n",
       " 115,\n",
       " 13,\n",
       " 12,\n",
       " 64,\n",
       " 21,\n",
       " 53,\n",
       " 7,\n",
       " 792,\n",
       " 1,\n",
       " 887,\n",
       " 1415,\n",
       " 48,\n",
       " 4,\n",
       " 282,\n",
       " 18,\n",
       " 1299,\n",
       " 180,\n",
       " 1571,\n",
       " 12,\n",
       " 67,\n",
       " 3557,\n",
       " 16,\n",
       " 31,\n",
       " 276,\n",
       " 4,\n",
       " 14,\n",
       " 11,\n",
       " 28,\n",
       " 456,\n",
       " 2,\n",
       " 97,\n",
       " 471,\n",
       " 68,\n",
       " 11,\n",
       " 97,\n",
       " 617,\n",
       " 281,\n",
       " 40,\n",
       " 36,\n",
       " 61,\n",
       " 50,\n",
       " 244,\n",
       " 2,\n",
       " 190,\n",
       " 61,\n",
       " 50,\n",
       " 1495,\n",
       " 193,\n",
       " 409,\n",
       " 2,\n",
       " 80,\n",
       " 29,\n",
       " 294,\n",
       " 457,\n",
       " 58,\n",
       " 2,\n",
       " 57,\n",
       " 3,\n",
       " 269,\n",
       " 1,\n",
       " 105,\n",
       " 94,\n",
       " 144,\n",
       " 3,\n",
       " 673,\n",
       " 25,\n",
       " 12,\n",
       " 6,\n",
       " 445,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6606285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090fa908",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Building model\n",
    "##### Vectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e13fa93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_layer = TextVectorization(max_tokens=total_words,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=window_size)\n",
    "\n",
    "vectorizer_layer.adapt(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48961a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "embedding_layer = tf.keras.layers.Embedding(total_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be52eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(1,), dtype=tf.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b74f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70dd8ad1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.add(input_layer)\n",
    "# model.add(vectorizer_layer.input_layer())\n",
    "# model.add(embedding_layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33e2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bcc06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n",
    "# x = (tf.keras.layers.Dense(4))\n",
    "# outputs = tf.keras.layers.Dense(total_words, activation=\"softmax\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c8a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db428eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector = vectorizer_layer(input_layer)\n",
    "embedd = embedding_layer(text_vector)\n",
    "embedd_outputs = layers.Dense(128, activation=\"relu\")(embedd)\n",
    "outputs = layers.Dense(total_words, activation=\"softmax\")(embedd_outputs)\n",
    "model = tf.keras.Model(input_layer, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca25c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1083, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 2005, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, 10, 8032) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15608\\2178038700.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the token, char and positional embedding model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1083, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\losses.py\", line 2005, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis\n    File \"c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, 10, 8032) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Fit the token, char and positional embedding model\n",
    "history_model = model.fit(dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69605a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c81911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the token, char, positional embedding model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb11f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cfbd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_data = [(\"in olden times when wishing still helped one\"),(\"true\")]\n",
    "# model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d80dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = model.predict(input_data)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'in olden times when wishing still helped one, there lived a king  whose daughters were all beautiful, but the youngest was so beautiful  that the sun itself, which has seen so much, was astonished whenever  it shone in her face.  close by the kings castle lay a great dark  forest, and under an old limetree in the forest was a well, and when  the day was very warm, the kings child went out into the forest and  sat down by the side of the cool fountain, and when she was bored she  took a golden ball, and threw it up on high and caught it, and this  ball was her favorite plaything.    now it so happened that on one occasion the princesss golden ball  did not fall into the little hand which she was holding up for it,  but on to the ground beyond, and rolled straight into the water.  the  kings daughter followed it with her eyes, but it vanished, and the  well was deep, so deep that the bottom could not be seen.  at this  she began to cry, and cried louder and louder, and could not be  comf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2639a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d426687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9545d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc4dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50e0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d492905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c9227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784fd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58162994",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f1d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85697829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fc0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text, target_text = chunk[:-1], chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = create_dataset(text_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = create_dataset([1,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f850c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in dataset2.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b2e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a828bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cfef5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences = [[73, 313],\n",
    " [73, 313, 616],\n",
    " [73, 313, 616, 3],\n",
    " [73, 313, 616, 3, 617],\n",
    " [73, 313, 616, 3, 617, 205],\n",
    " [73, 313, 616, 3, 617, 205, 314],\n",
    " [618, 38],\n",
    " [618, 38, 619],\n",
    " [618, 38, 619, 1],\n",
    " [618, 38, 619, 1, 206]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "625ec424",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22928\\3553097337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1abdc66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22928\\250114734.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_padded_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmax_sequence_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_sequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minput_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\users\\ntviet5\\desktop\\tensorflow-certificate\\env\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee98a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e40b7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "#     text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\n",
    "    vocab = sorted(set(text))  # The unique characters in the file\n",
    "    # Creating a mapping from unique characters to indices and vice versa\n",
    "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[c] for c in text])\n",
    "    return text_as_int, vocab, char2idx, idx2char\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text, target_text = chunk[:-1], chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=64):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=1000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    # Low temperatures results in more predictable text, higher temperatures results in more surprising text.\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    start_eval = input_eval[:]\n",
    "#     print(input_eval)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "#     print(input_eval)\n",
    "    text_generated = []  # Empty string to store our results\n",
    "    model.reset_states()\n",
    "    for i in range(generate_char_num):\n",
    "        predictions = model(input_eval)\n",
    "#         print(predictions)\n",
    "        predictions = tf.squeeze(predictions, 0)    # remove the batch dimension\n",
    "#         print(predictions)\n",
    "        predictions /= temperature\n",
    "#         print(predictions)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "#         print(predicted_id)\n",
    "        start_eval.append(predicted_id)\n",
    "        # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims(start_eval[-100:], axis=0)\n",
    "#         input_eval = tf.expand_dims([predicted_id], axis=0)\n",
    "#         print(input_eval)\n",
    "#         print(idx2char[predicted_id])\n",
    "#         print('---------------')\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "\n",
    "# path_to_file = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\n",
    "# dataset = create_dataset(text_as_int)\n",
    "# model = build_model(vocab_size=len(vocab))\n",
    "# model.compile(optimizer='adam', loss=loss)\n",
    "# model.summary()\n",
    "# history = model.fit(dataset, epochs=50)\n",
    "# model.save_weights(\"gen_text_weights.h5\", save_format='h5')\n",
    "# # To keep this prediction step simple, use a batch size of 1\n",
    "# model = build_model(vocab_size=len(vocab), batch_size=1)\n",
    "# model.load_weights(\"gen_text_weights.h5\")\n",
    "# model.summary()\n",
    "\n",
    "# user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "# generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=2000)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53fa9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int, vocab, char2idx, idx2char = process_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a08efc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of the text, the program will complete it. Your input is: test\n",
      "test companied only by one  servant, rode full gallop to the forest.  the servant fell with  his horse, \n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=100)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc5a1f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of the text, the program will complete it. Your input is: the kings daughter was sitting\n",
      "the kings daughter was sitting, when he went in  through the basket, the youth became king and  queen, and went some you.  my wish\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=100)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23890a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of the text, the program will complete it. Your input is: kings daughter was sitting\n",
      "kings daughter was sitting upon the  heart.  that is the morning, said the jew, and fell asleep.  cock  she had no children.  once when the prince was riding forth the  birds   wondered at this shape and looked at her, so that she could not refuse.    after that the aged grandmother had already was a powerful golden feather orest three kings daughter come, and if you dont know her, i have  got the cow  for your daughter as your rame more place.  on this dress, and when they he window that she  had vanished, and now the b\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=500)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "61b2a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (64, None, 256)           9216      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_17 (Dense)            (64, None, 36)            36900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,693,988\n",
      "Trainable params: 13,689,892\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "228/228 [==============================] - 839s 4s/step - loss: 1.9685\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - 870s 4s/step - loss: 1.3592\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - 882s 4s/step - loss: 1.2406\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - 857s 4s/step - loss: 1.1756\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - 880s 4s/step - loss: 1.1274\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - 876s 4s/step - loss: 1.0860\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - 860s 4s/step - loss: 1.0486\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - 834s 4s/step - loss: 1.0133\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - 847s 4s/step - loss: 0.9777\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - 808s 4s/step - loss: 0.9433\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - 842s 4s/step - loss: 0.9084\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - 874s 4s/step - loss: 0.8736\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - 861s 4s/step - loss: 0.8391\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - 826s 4s/step - loss: 0.8050\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.7724\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.7409\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - 803s 4s/step - loss: 0.7110\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.6820\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.6555\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.6292\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.6074\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.5860\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.5656\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.5477\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.5308\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.5162\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.5022\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.4899\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.4790\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.4685\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.4582\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.4490\n",
      "Epoch 33/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.4404\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.4345\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - 803s 4s/step - loss: 0.4273\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.4195\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.4140\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - 803s 4s/step - loss: 0.4094\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.4022\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.3982\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.3940\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.3894\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.3850\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.3807\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - 807s 4s/step - loss: 0.3772\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.3737\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - 803s 4s/step - loss: 0.3702\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - 806s 4s/step - loss: 0.3676\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - 805s 4s/step - loss: 0.3648\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - 804s 4s/step - loss: 0.3617\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (1, None, 256)            9216      \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (1, None, 1024)           0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (1, None, 1024)          4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (1, None, 1024)           8392704   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (1, None, 1024)           0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (1, None, 1024)          4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_18 (Dense)            (1, None, 36)             36900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,693,988\n",
      "Trainable params: 13,689,892\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(text_as_int)\n",
    "model = build_model(vocab_size=len(vocab))\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.summary()\n",
    "history = model.fit(dataset, epochs=50)\n",
    "# model.save_weights(\"gen_text_weights.h5\", save_format='h5')\n",
    "# To keep this prediction step simple, use a batch size of 1\n",
    "model = build_model(vocab_size=len(vocab), batch_size=1)\n",
    "model.load_weights(\"gen_text_weights.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64dfa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "# generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=2000)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01915798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 38, 39, 11]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [39, 24, 38, 39, 11]\n",
    "a[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02991c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def process_textx(text):\n",
    "# #     text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\n",
    "#     vocab = sorted(set(text))  # The unique characters in the file\n",
    "#     # Creating a mapping from unique characters to indices and vice versa\n",
    "#     char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "#     idx2char = np.array(vocab)\n",
    "#     text_as_int = np.array([char2idx[c] for c in text])\n",
    "#     return text_as_int, vocab, char2idx, idx2char\n",
    "\n",
    "\n",
    "# def split_input_targetx(chunk):\n",
    "#     input_text, target_text = chunk[:-1], chunk[1:]\n",
    "#     return input_text, target_text\n",
    "\n",
    "\n",
    "# def create_datasetx(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
    "#     char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "#     dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
    "# #     dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79ca22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Tensor(\"args_0:0\", shape=(101,), dtype=int32)\n",
      "Tensor(\"strided_slice:0\", shape=(100,), dtype=int32) Tensor(\"strided_slice_1:0\", shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tt = create_dataset(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1272b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f2b8a329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "13ce30b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
       "  array([[25, 27, 24, ..., 34, 24, 30],\n",
       "         [ 1, 34, 24, ..., 21, 13,  1],\n",
       "         [ 6,  1,  1, ..., 13,  1, 29],\n",
       "         ...,\n",
       "         [ 5,  1, 11, ..., 28,  1, 10],\n",
       "         [17, 10, 29, ..., 17, 10, 29],\n",
       "         [14, 29,  1, ...,  1, 32, 17]])>,\n",
       "  <tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
       "  array([[27, 24, 25, ..., 24, 30, 27],\n",
       "         [34, 24, 30, ..., 13,  1, 23],\n",
       "         [ 1,  1, 29, ...,  1, 29, 17],\n",
       "         ...,\n",
       "         [ 1, 11, 30, ...,  1, 10, 27],\n",
       "         [10, 29,  1, ..., 10, 29,  1],\n",
       "         [29,  1, 22, ..., 32, 17, 18]])>)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86500b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8d52e694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "36fce465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e5a4951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "68aefa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f156d7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11175683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4306b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (64, None, 256)           9216      \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (64, None, 36)            36900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,693,988\n",
      "Trainable params: 13,689,892\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size=len(vocab))\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.load_weights(\"gen_text_weights.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82b00064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of the text, the program will complete it. Your input is: test\n",
      "tf.Tensor(\n",
      "[[[ -5.5037856    4.7511535   -4.579504    -1.3326935   -5.997266\n",
      "     3.8355663    1.6708739   -3.6531289   -1.9414227   -7.3208733\n",
      "     2.4477441   -4.4496236   -2.0152292   -4.228157     2.819153\n",
      "    -4.323568    -2.8293428    6.5398674    1.5040026   -5.153716\n",
      "    -3.8815207    2.9991596   -2.0816274   -4.3387246    4.2962947\n",
      "    -5.3018045   -7.454533     1.2362175   -0.20588052   0.41155413\n",
      "     1.0430781   -4.949195     3.6240258   -4.062361     0.03601163\n",
      "    -4.9055543 ]\n",
      "  [ -7.05791      3.8161633   -6.500891    -3.6414137   -5.3209558\n",
      "     1.1802993    0.5248253   -5.892267    -3.1630824   -8.50098\n",
      "     1.1969467    0.66362655  -0.6623765    3.1048393    1.2327492\n",
      "    -2.3829548   -2.680851    -2.771061    -0.8635484   -4.319003\n",
      "    -1.6912453    4.0789       2.1742647    3.0265334   -1.1597085\n",
      "     0.623724    -5.1353116    5.432199     1.9001968   -0.32841235\n",
      "    -1.924179    -1.0967962   -1.56865     -1.9725931   -1.0428154\n",
      "    -5.7373805 ]\n",
      "  [ -8.016561     4.9756217   -7.50945     -4.9084563   -8.971946\n",
      "     3.2346451    3.191484    -7.1401076   -4.2698803  -10.484316\n",
      "    -0.938275    -4.9445796    0.76455665   0.75494874   2.3063478\n",
      "     0.86916846  -0.82385904   0.5931189   -0.47068027  -7.4636617\n",
      "    -2.0286157    0.49878576   3.4350405   -0.48246685  -2.2074254\n",
      "    -0.3776931   -7.197211     1.2988281    5.87193      4.5552063\n",
      "     0.0879988   -2.0404243   -0.5363696   -5.5347977   -0.23617357\n",
      "    -4.978324  ]\n",
      "  [-10.029531     6.9533877   -9.26782     -4.441684   -10.273099\n",
      "     5.7330937    4.545417   -10.313198    -1.411298   -11.288151\n",
      "     0.7608199   -3.960914    -1.2046527   -1.5071234    3.993749\n",
      "    -0.6340867   -2.8020203    1.2767738    3.39132     -8.346076\n",
      "    -5.3028684    2.1280906   -1.4417866    0.44608027   1.1041408\n",
      "    -2.4157274   -9.989201     1.3148746    1.4676347    0.7182005\n",
      "     1.2564274   -2.239346    -3.246436    -6.5900784    1.4088414\n",
      "    -7.8655243 ]]], shape=(1, 4, 36), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ -5.5037856    4.7511535   -4.579504    -1.3326935   -5.997266\n",
      "    3.8355663    1.6708739   -3.6531289   -1.9414227   -7.3208733\n",
      "    2.4477441   -4.4496236   -2.0152292   -4.228157     2.819153\n",
      "   -4.323568    -2.8293428    6.5398674    1.5040026   -5.153716\n",
      "   -3.8815207    2.9991596   -2.0816274   -4.3387246    4.2962947\n",
      "   -5.3018045   -7.454533     1.2362175   -0.20588052   0.41155413\n",
      "    1.0430781   -4.949195     3.6240258   -4.062361     0.03601163\n",
      "   -4.9055543 ]\n",
      " [ -7.05791      3.8161633   -6.500891    -3.6414137   -5.3209558\n",
      "    1.1802993    0.5248253   -5.892267    -3.1630824   -8.50098\n",
      "    1.1969467    0.66362655  -0.6623765    3.1048393    1.2327492\n",
      "   -2.3829548   -2.680851    -2.771061    -0.8635484   -4.319003\n",
      "   -1.6912453    4.0789       2.1742647    3.0265334   -1.1597085\n",
      "    0.623724    -5.1353116    5.432199     1.9001968   -0.32841235\n",
      "   -1.924179    -1.0967962   -1.56865     -1.9725931   -1.0428154\n",
      "   -5.7373805 ]\n",
      " [ -8.016561     4.9756217   -7.50945     -4.9084563   -8.971946\n",
      "    3.2346451    3.191484    -7.1401076   -4.2698803  -10.484316\n",
      "   -0.938275    -4.9445796    0.76455665   0.75494874   2.3063478\n",
      "    0.86916846  -0.82385904   0.5931189   -0.47068027  -7.4636617\n",
      "   -2.0286157    0.49878576   3.4350405   -0.48246685  -2.2074254\n",
      "   -0.3776931   -7.197211     1.2988281    5.87193      4.5552063\n",
      "    0.0879988   -2.0404243   -0.5363696   -5.5347977   -0.23617357\n",
      "   -4.978324  ]\n",
      " [-10.029531     6.9533877   -9.26782     -4.441684   -10.273099\n",
      "    5.7330937    4.545417   -10.313198    -1.411298   -11.288151\n",
      "    0.7608199   -3.960914    -1.2046527   -1.5071234    3.993749\n",
      "   -0.6340867   -2.8020203    1.2767738    3.39132     -8.346076\n",
      "   -5.3028684    2.1280906   -1.4417866    0.44608027   1.1041408\n",
      "   -2.4157274   -9.989201     1.3148746    1.4676347    0.7182005\n",
      "    1.2564274   -2.239346    -3.246436    -6.5900784    1.4088414\n",
      "   -7.8655243 ]], shape=(4, 36), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ -5.5037856    4.7511535   -4.579504    -1.3326935   -5.997266\n",
      "    3.8355663    1.6708739   -3.6531289   -1.9414227   -7.3208733\n",
      "    2.4477441   -4.4496236   -2.0152292   -4.228157     2.819153\n",
      "   -4.323568    -2.8293428    6.5398674    1.5040026   -5.153716\n",
      "   -3.8815207    2.9991596   -2.0816274   -4.3387246    4.2962947\n",
      "   -5.3018045   -7.454533     1.2362175   -0.20588052   0.41155413\n",
      "    1.0430781   -4.949195     3.6240258   -4.062361     0.03601163\n",
      "   -4.9055543 ]\n",
      " [ -7.05791      3.8161633   -6.500891    -3.6414137   -5.3209558\n",
      "    1.1802993    0.5248253   -5.892267    -3.1630824   -8.50098\n",
      "    1.1969467    0.66362655  -0.6623765    3.1048393    1.2327492\n",
      "   -2.3829548   -2.680851    -2.771061    -0.8635484   -4.319003\n",
      "   -1.6912453    4.0789       2.1742647    3.0265334   -1.1597085\n",
      "    0.623724    -5.1353116    5.432199     1.9001968   -0.32841235\n",
      "   -1.924179    -1.0967962   -1.56865     -1.9725931   -1.0428154\n",
      "   -5.7373805 ]\n",
      " [ -8.016561     4.9756217   -7.50945     -4.9084563   -8.971946\n",
      "    3.2346451    3.191484    -7.1401076   -4.2698803  -10.484316\n",
      "   -0.938275    -4.9445796    0.76455665   0.75494874   2.3063478\n",
      "    0.86916846  -0.82385904   0.5931189   -0.47068027  -7.4636617\n",
      "   -2.0286157    0.49878576   3.4350405   -0.48246685  -2.2074254\n",
      "   -0.3776931   -7.197211     1.2988281    5.87193      4.5552063\n",
      "    0.0879988   -2.0404243   -0.5363696   -5.5347977   -0.23617357\n",
      "   -4.978324  ]\n",
      " [-10.029531     6.9533877   -9.26782     -4.441684   -10.273099\n",
      "    5.7330937    4.545417   -10.313198    -1.411298   -11.288151\n",
      "    0.7608199   -3.960914    -1.2046527   -1.5071234    3.993749\n",
      "   -0.6340867   -2.8020203    1.2767738    3.39132     -8.346076\n",
      "   -5.3028684    2.1280906   -1.4417866    0.44608027   1.1041408\n",
      "   -2.4157274   -9.989201     1.3148746    1.4676347    0.7182005\n",
      "    1.2564274   -2.239346    -3.246436    -6.5900784    1.4088414\n",
      "   -7.8655243 ]], shape=(4, 36), dtype=float32)\n",
      "5\n",
      ",\n",
      "---------------\n",
      "test,\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=1)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ed9ccae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Tensor(\"args_0:0\", shape=(101,), dtype=int32)\n",
      "Tensor(\"strided_slice:0\", shape=(100,), dtype=int32) Tensor(\"strided_slice_1:0\", shape=(100,), dtype=int32)\n",
      "Epoch 1/5\n",
      "228/228 [==============================] - 846s 4s/step - loss: 0.3667\n",
      "Epoch 2/5\n",
      "228/228 [==============================] - 860s 4s/step - loss: 0.3585\n",
      "Epoch 3/5\n",
      "228/228 [==============================] - 816s 4s/step - loss: 0.3551\n",
      "Epoch 4/5\n",
      "228/228 [==============================] - 801s 4s/step - loss: 0.3524\n",
      "Epoch 5/5\n",
      "228/228 [==============================] - 801s 4s/step - loss: 0.3506\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (64, None, 256)           9216      \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (64, None, 1024)          0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (64, None, 1024)         4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (64, None, 36)            36900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,693,988\n",
      "Trainable params: 13,689,892\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(text_as_int)\n",
    "history = model.fit(dataset, epochs=5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3e6232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"gen_text_weights.h5\", save_format='h5')\n",
    "# To keep this prediction step simple, use a batch size of 1\n",
    "model = build_model(vocab_size=len(vocab), batch_size=1)\n",
    "model.load_weights(\"gen_text_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1ed6c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[16, 28,  1, ..., 17, 10, 28],\n",
      "       [17, 14,  1, ..., 22, 34, 28],\n",
      "       [14, 23,  1, ..., 16, 24, 21],\n",
      "       ...,\n",
      "       [28, 10, 29, ..., 29,  1, 28],\n",
      "       [28,  5,  1, ..., 31, 14, 27],\n",
      "       [29, 24,  1, ..., 17, 14, 27]])>, <tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[28,  1, 13, ..., 10, 28,  1],\n",
      "       [14,  1, 25, ..., 34, 28, 14],\n",
      "       [23,  1, 18, ..., 24, 21, 13],\n",
      "       ...,\n",
      "       [10, 29,  1, ...,  1, 28, 17],\n",
      "       [ 5,  1,  3, ..., 14, 27,  5],\n",
      "       [24,  1, 20, ..., 14, 27, 28]])>)\n",
      "(<tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[14, 10, 27, ..., 14, 13,  1],\n",
      "       [27, 22, 14, ...,  1,  1, 10],\n",
      "       [26, 30, 14, ..., 25, 10, 21],\n",
      "       ...,\n",
      "       [24, 23, 16, ..., 10, 21, 21],\n",
      "       [29, 24,  1, ...,  1, 13, 24],\n",
      "       [21, 21, 14, ..., 31, 18, 12]])>, <tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[10, 27, 14, ..., 13,  1, 29],\n",
      "       [22, 14, 27, ...,  1, 10,  1],\n",
      "       [30, 14, 14, ..., 10, 21, 10],\n",
      "       ...,\n",
      "       [23, 16,  1, ..., 21, 21, 14],\n",
      "       [24,  1, 29, ..., 13, 24, 24],\n",
      "       [21, 14,  6, ..., 18, 12, 14]])>)\n",
      "(<tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[29, 17, 14, ..., 27, 14, 13],\n",
      "       [17, 10, 29, ..., 34,  1, 34],\n",
      "       [23, 13,  1, ..., 29,  5,  1],\n",
      "       ...,\n",
      "       [10, 31, 14, ..., 30,  1, 32],\n",
      "       [28, 10, 18, ...,  6,  1,  1],\n",
      "       [14, 19, 24, ..., 14, 34,  1]])>, <tf.Tensor: shape=(64, 100), dtype=int32, numpy=\n",
      "array([[17, 14, 27, ..., 14, 13,  1],\n",
      "       [10, 29,  1, ...,  1, 34, 24],\n",
      "       [13,  1, 22, ...,  5,  1, 17],\n",
      "       ...,\n",
      "       [31, 14,  1, ...,  1, 32, 18],\n",
      "       [10, 18, 13, ...,  1,  1, 17],\n",
      "       [19, 24, 18, ..., 34,  1, 10]])>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x) for x in dataset.take(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a45871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c4292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8b164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6aef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

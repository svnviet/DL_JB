{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for text generation: Here I will try to predict new text based on the existing text data using Long-short-term-memory aka LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data: extracting the data from a website.It has 209 stories which are translated into english from german, making use of urlretrieve and os lib to download and structure the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    return filepath\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 209\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "# Download each file\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "    \n",
    "# Check if all files are downloaded\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print(f\"{len(filenames)} files found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the data: Now splitting the data into train,test and validation sets and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 167 files in the train dataset (e.g. ['data\\\\117.txt', 'data\\\\133.txt', 'data\\\\069.txt'])\n",
      "Got 21 files in the valid dataset (e.g. ['data\\\\023.txt', 'data\\\\078.txt', 'data\\\\176.txt'])\n",
      "Got 21 files in the test dataset (e.g. ['data\\\\129.txt', 'data\\\\207.txt', 'data\\\\170.txt'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Fix the random seed so we get the same outptu everytime\n",
    "random_state = 54321\n",
    "filenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\n",
    "# First separate train and valid+test data\n",
    "train_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=0.2, random_state=random_state)\n",
    "# Separate valid+test data to validation and test data\n",
    "valid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=0.5, random_state=random_state) \n",
    "# Print size of different subsets\n",
    "for subset_id, subset in zip(('train', 'valid', 'test'), (train_filenames, valid_filenames, test_filenames)):\n",
    "    print(f\"Got {len(subset)} files in the {subset_id} dataset (e.g. {subset[:3]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 705 unique bigrams\n"
     ]
    }
   ],
   "source": [
    "# defining a bigram set\n",
    "bigram_set = set()\n",
    "# Go through each file in the training set\n",
    "for fname in train_filenames:\n",
    "    # This will hold all the text\n",
    "    document = [] \n",
    "    with open(fname, 'r') as f:\n",
    "        for row in f:\n",
    "            # Convert text to lower case to reduce input dimensionality\n",
    "            document.append(row.lower())\n",
    "        # From the list of text we have create a single list having all stories\n",
    "        document = \" \".join(document)\n",
    "        # Update the set with all bigrams found\n",
    "        bigram_set.update([document[i:i+2] for i in range(0, len(document), 2)])\n",
    "# Assign to a variable\n",
    "n_vocab = len(bigram_set)\n",
    "print(f\"Found {n_vocab} unique bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'There was once upon a time a shepherd boy whose fame spread\\r\\nfar and wide because of the wise answers which he gave to every\\r\\nquestion.  The king of the country heard of it likewise, but\\r\\ndid not believe it, and sent for the boy.  Then he said to\\r\\nhim, if you can give me an answer to three questions which I\\r\\nwill ask you, I will look on you as my own child, and you shall\\r\\ndwell with me in my royal palace.  The boy said, what are the\\r\\nthree questions.  The king said, the first is, how many drops\\r\\nof water are there in the ocean.  The shepherd boy answered, lord\\r\\nking, if you will have all the rivers on earth dammed up so that\\r\\nnot a single drop runs from them into the sea until I have\\r\\ncounted it, I will tell you how many drops there are in the sea.\\r\\nThe king said, the next question is, how many stars are there\\r\\nin the sky.  The shepherd boy said, give me a great sheet of\\r\\nwhite paper, and then he made so many fine points on it with a\\r\\npen that they could\\r\\nscarcely be seen, and it was all but impossible to count them,\\r\\nany one who looked at them would have lost his sight.  Then he\\r\\nsaid, there are as many stars in the sky as there are points\\r\\non the paper.  Just count them.  But no one was able to do it.\\r\\nThe king said, the third question is, how many seconds of time\\r\\nare there in eternity.  Then said the shepherd boy, in\\r\\nlower pomerania is the diamond mountain, which is two miles\\r\\nhigh, two miles wide, and two miles deep.  Every hundred\\r\\nyears a little bird comes and sharpens its beak on it, and\\r\\nwhen the whole mountain is worn away by this, then the first\\r\\nsecond of eternity will be over.\\r\\nThe king said, you have answered the three questions like a\\r\\nwise man, and shall henceforth dwell with me in my royal\\r\\npalace, and I will regard you as my own child.\\r\\n', shape=(), dtype=string)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\117.txt',\n",
       " 'data\\\\133.txt',\n",
       " 'data\\\\069.txt',\n",
       " 'data\\\\195.txt',\n",
       " 'data\\\\107.txt',\n",
       " 'data\\\\183.txt',\n",
       " 'data\\\\066.txt',\n",
       " 'data\\\\074.txt',\n",
       " 'data\\\\105.txt',\n",
       " 'data\\\\087.txt',\n",
       " 'data\\\\152.txt',\n",
       " 'data\\\\068.txt',\n",
       " 'data\\\\111.txt',\n",
       " 'data\\\\168.txt',\n",
       " 'data\\\\153.txt',\n",
       " 'data\\\\166.txt',\n",
       " 'data\\\\080.txt',\n",
       " 'data\\\\088.txt',\n",
       " 'data\\\\197.txt',\n",
       " 'data\\\\042.txt',\n",
       " 'data\\\\154.txt',\n",
       " 'data\\\\185.txt',\n",
       " 'data\\\\196.txt',\n",
       " 'data\\\\120.txt',\n",
       " 'data\\\\142.txt',\n",
       " 'data\\\\186.txt',\n",
       " 'data\\\\030.txt',\n",
       " 'data\\\\155.txt',\n",
       " 'data\\\\045.txt',\n",
       " 'data\\\\188.txt',\n",
       " 'data\\\\109.txt',\n",
       " 'data\\\\191.txt',\n",
       " 'data\\\\043.txt',\n",
       " 'data\\\\001.txt',\n",
       " 'data\\\\124.txt',\n",
       " 'data\\\\085.txt',\n",
       " 'data\\\\163.txt',\n",
       " 'data\\\\100.txt',\n",
       " 'data\\\\127.txt',\n",
       " 'data\\\\032.txt',\n",
       " 'data\\\\146.txt',\n",
       " 'data\\\\156.txt',\n",
       " 'data\\\\081.txt',\n",
       " 'data\\\\016.txt',\n",
       " 'data\\\\184.txt',\n",
       " 'data\\\\075.txt',\n",
       " 'data\\\\137.txt',\n",
       " 'data\\\\194.txt',\n",
       " 'data\\\\054.txt',\n",
       " 'data\\\\009.txt',\n",
       " 'data\\\\179.txt',\n",
       " 'data\\\\206.txt',\n",
       " 'data\\\\209.txt',\n",
       " 'data\\\\034.txt',\n",
       " 'data\\\\103.txt',\n",
       " 'data\\\\116.txt',\n",
       " 'data\\\\131.txt',\n",
       " 'data\\\\138.txt',\n",
       " 'data\\\\178.txt',\n",
       " 'data\\\\126.txt',\n",
       " 'data\\\\036.txt',\n",
       " 'data\\\\167.txt',\n",
       " 'data\\\\193.txt',\n",
       " 'data\\\\135.txt',\n",
       " 'data\\\\125.txt',\n",
       " 'data\\\\089.txt',\n",
       " 'data\\\\147.txt',\n",
       " 'data\\\\102.txt',\n",
       " 'data\\\\003.txt',\n",
       " 'data\\\\150.txt',\n",
       " 'data\\\\122.txt',\n",
       " 'data\\\\007.txt',\n",
       " 'data\\\\096.txt',\n",
       " 'data\\\\149.txt',\n",
       " 'data\\\\164.txt',\n",
       " 'data\\\\198.txt',\n",
       " 'data\\\\076.txt',\n",
       " 'data\\\\095.txt',\n",
       " 'data\\\\012.txt',\n",
       " 'data\\\\018.txt',\n",
       " 'data\\\\205.txt',\n",
       " 'data\\\\165.txt',\n",
       " 'data\\\\113.txt',\n",
       " 'data\\\\052.txt',\n",
       " 'data\\\\065.txt',\n",
       " 'data\\\\157.txt',\n",
       " 'data\\\\108.txt',\n",
       " 'data\\\\047.txt',\n",
       " 'data\\\\026.txt',\n",
       " 'data\\\\005.txt',\n",
       " 'data\\\\203.txt',\n",
       " 'data\\\\114.txt',\n",
       " 'data\\\\051.txt',\n",
       " 'data\\\\061.txt',\n",
       " 'data\\\\040.txt',\n",
       " 'data\\\\134.txt',\n",
       " 'data\\\\041.txt',\n",
       " 'data\\\\050.txt',\n",
       " 'data\\\\202.txt',\n",
       " 'data\\\\006.txt',\n",
       " 'data\\\\055.txt',\n",
       " 'data\\\\104.txt',\n",
       " 'data\\\\118.txt',\n",
       " 'data\\\\017.txt',\n",
       " 'data\\\\021.txt',\n",
       " 'data\\\\161.txt',\n",
       " 'data\\\\073.txt',\n",
       " 'data\\\\158.txt',\n",
       " 'data\\\\028.txt',\n",
       " 'data\\\\090.txt',\n",
       " 'data\\\\173.txt',\n",
       " 'data\\\\091.txt',\n",
       " 'data\\\\058.txt',\n",
       " 'data\\\\067.txt',\n",
       " 'data\\\\029.txt',\n",
       " 'data\\\\106.txt',\n",
       " 'data\\\\094.txt',\n",
       " 'data\\\\071.txt',\n",
       " 'data\\\\160.txt',\n",
       " 'data\\\\010.txt',\n",
       " 'data\\\\123.txt',\n",
       " 'data\\\\077.txt',\n",
       " 'data\\\\035.txt',\n",
       " 'data\\\\208.txt',\n",
       " 'data\\\\004.txt',\n",
       " 'data\\\\121.txt',\n",
       " 'data\\\\008.txt',\n",
       " 'data\\\\064.txt',\n",
       " 'data\\\\145.txt',\n",
       " 'data\\\\086.txt',\n",
       " 'data\\\\187.txt',\n",
       " 'data\\\\172.txt',\n",
       " 'data\\\\022.txt',\n",
       " 'data\\\\002.txt',\n",
       " 'data\\\\177.txt',\n",
       " 'data\\\\070.txt',\n",
       " 'data\\\\083.txt',\n",
       " 'data\\\\053.txt',\n",
       " 'data\\\\093.txt',\n",
       " 'data\\\\063.txt',\n",
       " 'data\\\\072.txt',\n",
       " 'data\\\\115.txt',\n",
       " 'data\\\\119.txt',\n",
       " 'data\\\\159.txt',\n",
       " 'data\\\\204.txt',\n",
       " 'data\\\\015.txt',\n",
       " 'data\\\\014.txt',\n",
       " 'data\\\\062.txt',\n",
       " 'data\\\\136.txt',\n",
       " 'data\\\\190.txt',\n",
       " 'data\\\\031.txt',\n",
       " 'data\\\\141.txt',\n",
       " 'data\\\\192.txt',\n",
       " 'data\\\\039.txt',\n",
       " 'data\\\\140.txt',\n",
       " 'data\\\\099.txt',\n",
       " 'data\\\\128.txt',\n",
       " 'data\\\\011.txt',\n",
       " 'data\\\\200.txt',\n",
       " 'data\\\\201.txt',\n",
       " 'data\\\\019.txt',\n",
       " 'data\\\\079.txt',\n",
       " 'data\\\\151.txt',\n",
       " 'data\\\\097.txt',\n",
       " 'data\\\\027.txt',\n",
       " 'data\\\\139.txt',\n",
       " 'data\\\\082.txt']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds=generate_tf_dataset(train_filenames,ngram_length,window_size,batch_size,shuffle=True)\n",
    "train_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A total of 705 words found, it will be much more if instead of character level bigram, word is taken as a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tf.data pipeline\n",
    "def generate_tf_dataset(filenames,ngram_width,window_size,batch_size,shuffle=False):\n",
    "    \"\"\"\n",
    "    Generate batched data\n",
    "    \"\"\"\n",
    "    documents=[]\n",
    "    for f in filenames:\n",
    "        doc=tf.io.read_file(f)\n",
    "        print(doc)\n",
    "        print(type(doc))\n",
    "        break\n",
    "        doc=tf.strings.ngrams( # generating ngram from string\n",
    "            tf.strings.bytes_split( # splititng word into char and creating a list of chars\n",
    "                tf.strings.regex_replace( # replacing new line with space\n",
    "                    tf.strings.lower(doc),\"\\n\",\" \" # convert to lower case\n",
    "                )\n",
    "            ),ngram_width,separator=''\n",
    "        )\n",
    "        documents.append(doc.numpy().tolist())\n",
    "        # documents is a list of list of strings, where each string is a story\n",
    "        # generating a ragged tensor: A ragged tensor has dimensions used to accept arbitrarily sized inputs, \n",
    "        # in this case its not possible that all stories have same no of ngrams and there are long sequences\n",
    "        # of ngrams representing the stories so using ragged tensor to store that\n",
    "    documents=tf.ragged.constant(documents)\n",
    "    # creating a dataset where each row in ragged tensor is sample\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "    # removing the overlap here created by tf.strings.ngrams:\n",
    "    # so taking nth ngram in the sequence\n",
    "    doc_dataset=doc_dataset.map(lambda x:x[::ngram_width])\n",
    "    # need to generate windows from text:\n",
    "    # ex- ab,bc,cd,ef,fg,gh.... window_size=3,shift=1 gives-[ab,cd,ef],[cd,ef,gh]...\n",
    "    # to create shorter, fixed-length windowed sequences from each story:\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            size=window_size+1, shift=int(window_size * 0.75)\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(window_size+1, drop_remainder=True)\n",
    "        )\n",
    "    )\n",
    "    # from each window generate input and output sequence: take all ngrams except last as input \n",
    "    # and all ngrams except first as output/target so at each time step,model predict next ngram \n",
    "    # given all previous ngrams, some overlap also needed\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    # Shuffle the data if required\n",
    "    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if shuffle else doc_dataset\n",
    "    # Batch the data\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "#     Return the data\n",
    "    return doc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters and generate train,test and validation data\n",
    "ngram_length=2\n",
    "batch_size=128\n",
    "window_size=128\n",
    "train_ds=generate_tf_dataset(train_filenames,ngram_length,window_size,batch_size,shuffle=True)\n",
    "test_ds=generate_tf_dataset(test_filenames,ngram_length,window_size,batch_size)\n",
    "valid_ds=generate_tf_dataset(valid_filenames,ngram_length,window_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_ds:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'th', shape=(), dtype=string)\n",
      "tf.Tensor(b' u', shape=(), dtype=string)\n",
      "tf.Tensor(b' s', shape=(), dtype=string)\n",
      "tf.Tensor(b'wh', shape=(), dtype=string)\n",
      "tf.Tensor(b'ea', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for r in ds:\n",
    "    print(r[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ']] -> [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ' b'a ']]\n",
      "[[b' u' b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph']] -> [[b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph' b'er']]\n",
      "[[b' s' b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ']] -> [[b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ' b'fa']]\n",
      "[[b'wh' b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd\\r' b' f']] -> [[b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd\\r' b' f' b'ar']]\n",
      "[[b'ea' b'd\\r' b' f' b'ar' b' a' b'nd' b' w' b'id' b'e ' b'be']] -> [[b'd\\r' b' f' b'ar' b' a' b'nd' b' w' b'id' b'e ' b'be' b'ca']]\n"
     ]
    }
   ],
   "source": [
    "# generating some data\n",
    "ds = generate_tf_dataset(train_filenames, 2, window_size=10, batch_size=1).take(5)\n",
    "for record in ds:\n",
    "    print(record[0].numpy(), '->', record[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First defining the tokenization layer and integrating it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.backend as K\n",
    "text_vectorizer=layers.TextVectorization(max_tokens=n_vocab,standardize=None,split=None,input_shape=(window_size,))\n",
    "# train model on data\n",
    "text_vectorizer.adapt(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ' h', ', ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few bigrams learnt by the text vectprization layer\n",
    "text_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,test and valid sets need to be processed:converting from string to ngram ids\n",
    "train_ds=train_ds.map(lambda x,y:(x,text_vectorizer(y)))\n",
    "test_ds=test_ds.map(lambda x,y:(x,text_vectorizer(y)))\n",
    "valid_ds=valid_ds.map(lambda x,y:(x,text_vectorizer(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model: It has previously trained Textvectorization layer, embedding layer,\n",
    "# two LSTM layers, a fully connected layer with ReLU and a final prediction layer with softmax\n",
    "lm_model=models.Sequential([\n",
    "    text_vectorizer,layers.Embedding(n_vocab+2,96),\n",
    "    layers.LSTM(512,return_state=False,return_sequences=True),\n",
    "    layers.LSTM(256,return_state=False,return_sequences=True),\n",
    "    layers.Dense(1024,activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_vocab,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### return_state=False means layer output only final output and if true,it return final output with state output, if its set true for LSTM it returns final output,cell state and hidden state  \n",
    "##### return_sequences=True cause layer to output full output sequence opposed to final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_2 (TextV  (None, 128)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 128, 96)           67872     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 128, 512)          1247232   \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128, 256)          787456    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128, 1024)         263168    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128, 1024)         0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128, 705)          722625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the perplexity metric:\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "    \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile model using  \n",
    "Sparse categorical cross-entropy as loss function  \n",
    "Adam as optimizer  \n",
    "Accuracy and perplexity as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy', PerplexityMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 96s 2s/step - loss: 5.4085 - accuracy: 0.0295 - perplexity: 247.5033 - val_loss: 5.1771 - val_accuracy: 0.0336 - val_perplexity: 178.3953\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 88s 2s/step - loss: 5.1944 - accuracy: 0.0324 - perplexity: 181.8748 - val_loss: 5.1432 - val_accuracy: 0.0488 - val_perplexity: 172.5667\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 90s 2s/step - loss: 5.0877 - accuracy: 0.0393 - perplexity: 163.9533 - val_loss: 4.8785 - val_accuracy: 0.0683 - val_perplexity: 132.7870\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 89s 2s/step - loss: 4.7160 - accuracy: 0.0934 - perplexity: 123.5397 - val_loss: 4.4629 - val_accuracy: 0.1201 - val_perplexity: 88.1136\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 92s 2s/step - loss: 4.3522 - accuracy: 0.1284 - perplexity: 79.6945 - val_loss: 4.1098 - val_accuracy: 0.1568 - val_perplexity: 62.0960\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 89s 2s/step - loss: 4.0588 - accuracy: 0.1676 - perplexity: 59.4026 - val_loss: 3.8771 - val_accuracy: 0.1916 - val_perplexity: 49.2481\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 86s 2s/step - loss: 3.8517 - accuracy: 0.1949 - perplexity: 48.2285 - val_loss: 3.6911 - val_accuracy: 0.2106 - val_perplexity: 40.9344\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 85s 2s/step - loss: 3.6923 - accuracy: 0.2153 - perplexity: 41.1354 - val_loss: 3.5619 - val_accuracy: 0.2307 - val_perplexity: 36.0473\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 85s 2s/step - loss: 3.5741 - accuracy: 0.2320 - perplexity: 36.5506 - val_loss: 3.4471 - val_accuracy: 0.2454 - val_perplexity: 32.1350\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 87s 2s/step - loss: 3.4781 - accuracy: 0.2434 - perplexity: 33.2277 - val_loss: 3.3662 - val_accuracy: 0.2535 - val_perplexity: 29.6775\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 86s 2s/step - loss: 3.4021 - accuracy: 0.2530 - perplexity: 30.8064 - val_loss: 3.3073 - val_accuracy: 0.2620 - val_perplexity: 28.0011\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 84s 2s/step - loss: 3.3361 - accuracy: 0.2599 - perplexity: 28.8577 - val_loss: 3.2415 - val_accuracy: 0.2691 - val_perplexity: 26.2427\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 94s 2s/step - loss: 3.2781 - accuracy: 0.2671 - perplexity: 27.2321 - val_loss: 3.1857 - val_accuracy: 0.2785 - val_perplexity: 24.8473\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 89s 2s/step - loss: 3.2256 - accuracy: 0.2742 - perplexity: 25.8576 - val_loss: 3.1388 - val_accuracy: 0.2854 - val_perplexity: 23.7233\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 87s 2s/step - loss: 3.1788 - accuracy: 0.2805 - perplexity: 24.6822 - val_loss: 3.0939 - val_accuracy: 0.2919 - val_perplexity: 22.6981\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 85s 2s/step - loss: 3.1345 - accuracy: 0.2863 - perplexity: 23.6137 - val_loss: 3.0574 - val_accuracy: 0.2972 - val_perplexity: 21.8950\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 84s 2s/step - loss: 3.0971 - accuracy: 0.2913 - perplexity: 22.7485 - val_loss: 3.0207 - val_accuracy: 0.3017 - val_perplexity: 21.1082\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 87s 2s/step - loss: 3.0586 - accuracy: 0.2971 - perplexity: 21.9061 - val_loss: 2.9847 - val_accuracy: 0.3073 - val_perplexity: 20.3699\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 87s 2s/step - loss: 3.0251 - accuracy: 0.3016 - perplexity: 21.1794 - val_loss: 2.9573 - val_accuracy: 0.3114 - val_perplexity: 19.8303\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 85s 2s/step - loss: 2.9927 - accuracy: 0.3067 - perplexity: 20.5081 - val_loss: 2.9234 - val_accuracy: 0.3176 - val_perplexity: 19.1909\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "lstm_history = lm_model.fit(train_ds, validation_data=valid_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we can see the accuracy is increasing and perplexity is decreasing, it will be much better if I ran it for more epochs but due to resource constraints, I could not do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 469ms/step - loss: 2.9792 - accuracy: 0.3071 - perplexity: 20.2580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.9792096614837646, 0.3070947527885437, 20.25795555114746]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating \n",
    "lm_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building inference model: defining a recursive model that takes the current time stepâ€™s output of the model as the input to the next time step.The need is to generate new text, nothing available in the beginning. Therefore,need to make adjustments to trained model, Using functional API not the sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_2 (TextVect  multiple            0           ['input_29[0][0]']               \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        multiple             67872       ['text_vectorization_2[3][0]']   \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 [(None, 1, 512),     1247232     ['embedding_2[3][0]',            \n",
      "                                 (None, 512),                     'input_30[0][0]',               \n",
      "                                 (None, 512)]                     'input_31[0][0]']               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_33 (InputLayer)          [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)                 [(None, 1, 256),     787456      ['lstm_14[0][0]',                \n",
      "                                 (None, 256),                     'input_32[0][0]',               \n",
      "                                 (None, 256)]                     'input_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                multiple             263168      ['lstm_15[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                multiple             722625      ['dense_4[3][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining inference model:\n",
    "inp=layers.Input(dtype=tf.string,shape=(1,))\n",
    "text_vectorized_out = lm_model.get_layer('text_vectorization_2')(inp)\n",
    "inp_state_c_lstm=layers.Input(shape=(512,))\n",
    "inp_state_h_lstm=layers.Input(shape=(512,))\n",
    "inp_state_c_lstm_1=layers.Input(shape=(256,))\n",
    "inp_state_h_lstm_1=layers.Input(shape=(256,))\n",
    "# Define embedding layer and output\n",
    "emb_layer=lm_model.get_layer('embedding_2')\n",
    "emb_out=emb_layer(text_vectorized_out)\n",
    "# Defining a LSTM layers and output\n",
    "lstm_layer=layers.LSTM(512,return_state=True,return_sequences=True)\n",
    "lstm_out,lstm_state_c,lstm_state_h=lstm_layer(emb_out,initial_state=[inp_state_c_lstm,inp_state_h_lstm])\n",
    "lstm_1_layer=tf.keras.layers.LSTM(256,return_state=True,return_sequences=True)\n",
    "lstm_1_out,lstm_1_state_c,lstm_1_state_h=lstm_1_layer(lstm_out,initial_state=[inp_state_c_lstm_1,inp_state_h_lstm_1])\n",
    "# Defining a Dense layer and output\n",
    "dense_out=lm_model.get_layer('dense_4')(lstm_1_out)\n",
    "# Defining the final Dense layer and output\n",
    "final_out=lm_model.get_layer('dense_5')(dense_out)\n",
    "# Copy the weights from the original model\n",
    "lstm_layer.set_weights(lm_model.get_layer('lstm_8').get_weights())\n",
    "lstm_1_layer.set_weights(lm_model.get_layer('lstm_9').get_weights())\n",
    "# Define final model\n",
    "infer_model=models.Model(\n",
    "    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, inp_state_c_lstm_1, inp_state_h_lstm_1], \n",
    "    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, lstm_1_state_h])\n",
    "# Summary\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using new inference model to generate a story. defining an initial seed that will be used to generate a story.Taking the the first phrase from one of the test files. Then usig it to generate text recursively, by using the predicted bigram at time t as the input at time t+1. Running for 500 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions from a 58 element long input\n",
      "1/1 [==============================] - 1s 660ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      " world, and the king's daughter the king's daughter.  the king's daughter that the king the king's daughter the king's daughter.  the king's daughter them, and the king's daughter the king's daughter the king's daughter the king's daughter.\" the king's daughter the king's daughter the king's daughter, and then there was the king's daughten the king's daughter the king's daughter th\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = [\"When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren ground\"]\n",
    "seq = [text[0][i:i+2] for i in range(0, len(text[0]), 2)]\n",
    "# build up model state using the given string\n",
    "print(f\"Making predictions from a {len(seq)} element long input\")\n",
    "vocabulary = infer_model.get_layer(\"text_vectorization_2\").get_vocabulary()\n",
    "index_word = dict(zip(range(len(vocabulary)), vocabulary))\n",
    "\n",
    "# Reset the state of the model initially\n",
    "infer_model.reset_states()\n",
    "# Definin the initial state as all zeros\n",
    "state_c = np.zeros(shape=(1,512))\n",
    "state_h = np.zeros(shape=(1,512))\n",
    "state_c_1 = np.zeros(shape=(1,256))\n",
    "state_h_1 = np.zeros(shape=(1,256))\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq:    \n",
    "    #print(c)\n",
    "    out, state_c, state_h, state_c_1, state_h_1 = infer_model.predict(\n",
    "        [np.array([[c]]), state_c, state_h, state_c_1, state_h_1]\n",
    ")\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = index_word[wid]\n",
    "text.append(word)\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[word]])\n",
    "for _ in range(500):    \n",
    "    # Get the next output and state\n",
    "    out, state_c, state_h, state_c_1, state_h_1  = infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ])\n",
    "    # Get the word id and the word from out\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()        \n",
    "    wid = int(out_argsort[-1])\n",
    "    word = index_word[wid]\n",
    "    # If the word ends with space, we introduce a bit of randomness\n",
    "    # Essentially pick one of the top 3 outputs for that timestep depending on their likelihood\n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 5\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = index_word[wid]\n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    # Recursively make the current prediction the next input\n",
    "    x = np.array([[word]])\n",
    "# Print the final output    \n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model is able to generate some meaningful text, it would be more better if I ran for more epochs ~ 100 but this is a small experiment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7773864c198c8559e499b8a6d42753464881661d1a635729c4702e1dcc7c46"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
